{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n4nHS95NXCUE",
        "aGqKSyzpWhUe",
        "cjMYoY5bnKvh",
        "5_E7YrQnVIsZ",
        "ba9NlMBe8ogU",
        "zoqVPbNE-hPM",
        "Xz93vMumrARI",
        "bQC5l-kfU9MB",
        "OYe1q4cJVEeh",
        "tFRLCOVQV2Z8",
        "CLYpXEUjgrsD",
        "xCgrhtN1E7qe",
        "W-BhhLBnd8YA",
        "7OL226o0k31t",
        "TLjo6q0PHMR4",
        "DhNMedE8QOct",
        "s9nQi303WFh6",
        "Zis7dL-p838x",
        "9tePBTVk86b2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moNcGL1-9iKP"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oONGG3gazkwt",
        "outputId": "6af6abd3-a4ea-4efe-b15e-65da6df3723a"
      },
      "source": [
        "!pip install gym-gridworlds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-gridworlds\n",
            "  Downloading gym_gridworlds-0.0.2-py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-gridworlds) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gym-gridworlds) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-gridworlds) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-gridworlds) (0.0.8)\n",
            "Installing collected packages: gym-gridworlds\n",
            "Successfully installed gym-gridworlds-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8HlMmP60RWw",
        "outputId": "c7962e10-e7b8-4b51-b0a3-99259871fd74"
      },
      "source": [
        "!pip install plotting"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plotting\n",
            "  Downloading plotting-0.0.7-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from plotting) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from plotting) (0.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from plotting) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->plotting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->plotting) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->plotting) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->plotting) (1.16.0)\n",
            "Installing collected packages: plotting\n",
            "Successfully installed plotting-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrESoBUiDoP",
        "outputId": "073e6654-4774-4157-8f37-4d027508b1f7"
      },
      "source": [
        "!rm -r ../multiagentparticleenvs/\n",
        "!git clone https://github.com/vlb9ae/multiagent-particle-envs.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '../multiagentparticleenvs/': No such file or directory\n",
            "Cloning into 'multiagent-particle-envs'...\n",
            "remote: Enumerating objects: 367, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 367 (delta 180), reused 170 (delta 170), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (367/367), 123.53 KiB | 4.41 MiB/s, done.\n",
            "Resolving deltas: 100% (229/229), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKW3QFQq-ta1"
      },
      "source": [
        "%mv multiagent-particle-envs/ multiagentparticleenvs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA5jsnVKiR2m",
        "outputId": "50482725-d588-43fc-bb7e-b56210fa30dc"
      },
      "source": [
        "%cd multiagentparticleenvs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/multiagentparticleenvs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqx9ZVojBwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe948ae1-725e-42f0-c0aa-3e89145d6e6a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  LICENSE.txt  make_env.py  multiagent  plotting_custom.py  README.md  setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syJrXIOElBfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daab27dd-01dd-4184-8cd5-21324fdc34d2"
      },
      "source": [
        "!pip install gym==0.10.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.10.5\n",
            "  Downloading gym-0.10.5.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.10.5) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.10.5) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym==0.10.5) (1.16.0)\n",
            "Collecting pyglet>=1.2.0 (from gym==0.10.5)\n",
            "  Downloading pyglet-2.0.15-py3-none-any.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.3/884.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.10.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.10.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.10.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.10.5) (2024.6.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581273 sha256=32eef62244d5c4709e59aa93cb9b3c9d64346acb0d05a702793cdc10a750b59a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/e4/3b/b3b32d8cdedd0e70545cc0a9139f3d66f8fd5d0c95d828d38e\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.10.5 pyglet-2.0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSEJouukk-Cb"
      },
      "source": [
        "import gym\n",
        "import gym_gridworlds\n",
        "import itertools\n",
        "import matplotlib\n",
        "import matplotlib.style\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJfUtbobmq90"
      },
      "source": [
        "from plotting_custom import EpisodeStats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUHBYNsyZwRv",
        "outputId": "06fa7379-bcb8-4ee9-ad37-15f4013c54fd"
      },
      "source": [
        "%cd multiagentparticleenvs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'multiagentparticleenvs/'\n",
            "/content/multiagentparticleenvs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq_x9eToyWUJ"
      },
      "source": [
        "from multiagentparticleenvs.make_env import make_env\n",
        "\n",
        "from collections import defaultdict\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# env = make_env('extra_landmark_speaker_listener')\n",
        "env = make_env('simpler_speaker_listener')\n",
        "#env = make_env('simple_speaker_listener')\n",
        "#env = make_env('simple')\n",
        "#env = gym.make('Cliff-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4nHS95NXCUE"
      },
      "source": [
        "#Epsilon Greedy Policy Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGDt40LC0HDd"
      },
      "source": [
        "def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based\n",
        "    on a given Q-function and epsilon.\n",
        "\n",
        "    Returns a function that takes the state\n",
        "    as an input and returns the probabilities\n",
        "    for each action in the form of a numpy array\n",
        "    of length of the action space(set of possible actions).\n",
        "    \"\"\"\n",
        "    def policyFunction(state):\n",
        "\n",
        "        Action_probabilities = np.ones(num_actions,\n",
        "                dtype = float) * epsilon / num_actions\n",
        "\n",
        "        best_action = np.argmax(Q[state])\n",
        "        Action_probabilities[best_action] += (1.0 - epsilon)\n",
        "        return Action_probabilities\n",
        "\n",
        "    return policyFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGqKSyzpWhUe"
      },
      "source": [
        "#UCB Policy Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuSOwqjVWege"
      },
      "source": [
        "def UCBPolicy(Q, N, c, t, state):\n",
        "\n",
        "  best_action = 0\n",
        "  best_q_val = float('-inf')\n",
        "  for i in range(len(N)):\n",
        "    if N[i] > 0:\n",
        "      cur_q_val = Q[state][i] + c*np.sqrt(np.log(t)/N[i])\n",
        "    else:\n",
        "      cur_q_val = Q[state][i]\n",
        "    if cur_q_val > best_q_val:\n",
        "      best_q_val = cur_q_val\n",
        "      best_action = i\n",
        "\n",
        "  return best_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjMYoY5bnKvh"
      },
      "source": [
        "#Communication counting helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ufO_UF0msnF"
      },
      "source": [
        "def comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW):\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "    if action == 0:\n",
        "      communication_count[0] += 1\n",
        "    if action == 1:\n",
        "      communication_count[1] += 1\n",
        "    if action == 2:\n",
        "      communication_count[2] += 1\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, GREEN):\n",
        "    if action == 0:\n",
        "      communication_count[3] += 1\n",
        "    if action == 1:\n",
        "      communication_count[4] += 1\n",
        "    if action == 2:\n",
        "      communication_count[5] += 1\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "    if action == 0:\n",
        "      communication_count[6] += 1\n",
        "    if action == 1:\n",
        "      communication_count[7] += 1\n",
        "    if action == 2:\n",
        "      communication_count[8] += 1\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, CYAN):\n",
        "    if action == 0:\n",
        "      communication_count[9] += 1\n",
        "    if action == 1:\n",
        "      communication_count[10] += 1\n",
        "    if action == 2:\n",
        "      communication_count[11] += 1\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, MAGENTA):\n",
        "    if action == 0:\n",
        "      communication_count[12] += 1\n",
        "    if action == 1:\n",
        "      communication_count[13] += 1\n",
        "    if action == 2:\n",
        "      communication_count[14] += 1\n",
        "  if np.array_equal(env.world.agents[0].goal_b.color, YELLOW):\n",
        "    if action == 0:\n",
        "      communication_count[15] += 1\n",
        "    if action == 1:\n",
        "      communication_count[16] += 1\n",
        "    if action == 2:\n",
        "      communication_count[17] += 1\n",
        "\n",
        "  return communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_E7YrQnVIsZ"
      },
      "source": [
        "#Normal qLearning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5erYmsGKAOVZ"
      },
      "source": [
        "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
        "                            alpha = 0.6, epsilon = 0.1):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm: Off-policy TD control.\n",
        "    Finds the optimal greedy policy while improving\n",
        "    following an epsilon-greedy policy\"\"\"\n",
        "\n",
        "    # Action value function\n",
        "    # A nested dictionary that maps\n",
        "    # state -> (action -> action-value).\n",
        "\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Keeps track of useful statistics\n",
        "    stats = EpisodeStats(\n",
        "        episode_lengths = np.zeros(num_episodes),\n",
        "        episode_rewards = np.zeros(num_episodes))\n",
        "\n",
        "\n",
        "    # Create an epsilon greedy policy function\n",
        "    # appropriately for environment action space\n",
        "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)\n",
        "\n",
        "    # For every episode\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        # Reset the environment and pick the first action\n",
        "        state = env.reset()\n",
        "        print(state)\n",
        "\n",
        "        for t in itertools.count():\n",
        "\n",
        "            # get probabilities of all actions from current state\n",
        "            action_probabilities = policy(state)\n",
        "\n",
        "            # choose action according to\n",
        "            # the probability distribution\n",
        "            action = np.random.choice(np.arange(\n",
        "                      len(action_probabilities)),\n",
        "                       p = action_probabilities)\n",
        "\n",
        "            # take action and get reward, transit to next state\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update statistics\n",
        "            stats.episode_rewards[ith_episode] += reward\n",
        "            stats.episode_lengths[ith_episode] = t\n",
        "\n",
        "            # TD Update\n",
        "            best_next_action = np.argmax(Q[next_state])\n",
        "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
        "            td_delta = td_target - Q[state][action]\n",
        "            Q[state][action] += alpha * td_delta\n",
        "\n",
        "            # done is True if episode terminated\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q, stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba9NlMBe8ogU"
      },
      "source": [
        "#UCB_Microsoft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MkfhRwS8rst"
      },
      "source": [
        "def UCB_Microsoft(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.ones(env.action_space[ind].n))))\n",
        "      N.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        #print(\"\\n\\nEpisode: \", ith_episode, \"\\n\\n\")\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "\n",
        "            action = np.argmax(q[tuple(state)])\n",
        "            #if action == 0 and ind == 1:\n",
        "            #  action = np.random.choice(range(1, env.action_space[ind].n))\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "              else:\n",
        "                action_probabilities[i] = 0\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          #if t == 0:\n",
        "          #  print(action_probs)\n",
        "\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "\n",
        "            #UCB-Hoeffding algorithm implementation with RUQL-style exponentiation of alpha terms\n",
        "            temp = n[tuple(state)][action] + 1\n",
        "            N[ind][tuple(state)][action] += 1\n",
        "\n",
        "            alpha = (20 + 1)/(20 + temp)\n",
        "\n",
        "            iota = np.log((25*env.action_space[ind].n*num_episodes*1000)/.1)\n",
        "\n",
        "            bonus = .1*np.sqrt(((20))/temp)\n",
        "            #q[tuple(state)][action] = \"\"\"((1 - alpha)**(1/(action_probs[ind][action])))*\"\"\"(1 - alpha)*q[tuple(state)][action] + \"\"\"(1 - (1-alpha)**(1/(action_probs[ind][action])))\"\"\"alpha*(reward[ind] + v[tuple(next_state[ind])] + bonus)\n",
        "            \"\"\"if ind == 1:\n",
        "              print(\"State: \", tuple(state))\n",
        "              print(\"Iota: \", iota)\n",
        "              print(\"Bonus: \", bonus)\n",
        "              print(\"Reward: \", reward[ind])\n",
        "              print(\"Alpha: \", alpha)\n",
        "              print(\"V-value: \", v[tuple(state)])\n",
        "              print(\"N-value: \", N[ind][tuple(state)][action])\"\"\"\n",
        "            Q[ind][tuple(state)][action] = (1 - alpha)*q[tuple(state)][action] + alpha*(reward[ind] + v[tuple(next_state[ind])])# + bonus)\n",
        "            V[ind][tuple(state)] = min(np.max(Q[ind][tuple(state)]), 20)\n",
        "            \"\"\"if ind == 1:\n",
        "              for a in range(env.action_space[ind].n):\n",
        "                print(\"Q-value for action \", a, \": \", Q[ind][tuple(state)][a])\"\"\"\n",
        "\n",
        "          prev_state = next_state\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10:\n",
        "            #print(\"Reached end of episode\")\n",
        "            #stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoqVPbNE-hPM"
      },
      "source": [
        "#UCB-Hoeffding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbnXwGMYkOSV"
      },
      "source": [
        "def UCB_Hoeffding(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (200*np.ones(env.action_space[ind].n))))\n",
        "      N.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          #if (ith_episode / num_episodes > cutoff):\n",
        "          #  cutoff += .1\n",
        "          #  epsilon /= 2\n",
        "            #env.world.agents[0].goal_b = np.random.choice(env.world.landmarks)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            #action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "            action = np.argmax(q[tuple(state)])\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "              else:\n",
        "                action_probabilities[i] = 0\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "\n",
        "            #UCB-Hoeffding algorithm implementation\n",
        "            temp = n[tuple(state)][action]\n",
        "            n[tuple(state)][action] += 1\n",
        "\n",
        "            if t > 0:\n",
        "              iota = np.log((25*env.action_space[ind].n*num_episodes*t)/.1)\n",
        "            else:\n",
        "              iota = np.log((25*env.action_space[ind].n*num_episodes)/.1)\n",
        "            bonus = 2*np.sqrt(((200**3)*iota)/temp)\n",
        "            #print(type(v[tuple(next_state[ind])]))\n",
        "            q[tuple(state)][action] = (1 - alpha)*q[tuple(state)][action] + alpha*(reward[ind] + v[tuple(next_state[ind])] + bonus)\n",
        "            #print(np.argmax(q[tuple(state)]))\n",
        "            v[tuple(state)] = np.argmax(q[tuple(state)])\n",
        "\n",
        "\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            #left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "            #right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "            #best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "            #td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "            #right_term *= td_target\n",
        "            #q[tuple(state)][action] = left_term + right_term\n",
        "\n",
        "          prev_state = next_state\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10:\n",
        "            #print(\"Reached end of episode\")\n",
        "            #stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz93vMumrARI"
      },
      "source": [
        "#UCB-Hoeffding with RUQL exponentiation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5ia8Uwaq9Er"
      },
      "source": [
        "def RUQL_UCB_Hoeffding(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (20*np.ones(env.action_space[ind].n))))\n",
        "      N.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        print(\"\\n\\nEpisode: \", ith_episode, \"\\n\\n\")\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "\n",
        "            action = np.argmax(q[tuple(state)])\n",
        "            if action == 0 and ind == 1:\n",
        "              action = np.random.choice(range(1, env.action_space[ind].n))\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "              else:\n",
        "                action_probabilities[i] = 0\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          #if t == 0:\n",
        "          #  print(action_probs)\n",
        "\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "\n",
        "            #UCB-Hoeffding algorithm implementation with RUQL-style exponentiation of alpha terms\n",
        "            temp = n[tuple(state)][action] + 1\n",
        "            N[ind][tuple(state)][action] += 1\n",
        "\n",
        "            alpha = (20 + 1)/(20 + temp)\n",
        "\n",
        "            iota = np.log((25*env.action_space[ind].n*num_episodes*1000)/.1)\n",
        "\n",
        "            bonus = 2*np.sqrt(((20**3)*iota)/temp)\n",
        "            q[tuple(state)][action] = ((1 - alpha)**(1/(action_probs[ind][action])))*(1 - alpha)*q[tuple(state)][action] + (1 - (1-alpha)**(1/(action_probs[ind][action])))*(reward[ind] + v[tuple(next_state[ind])] + bonus)\n",
        "\n",
        "            V[ind][tuple(state)] = min(np.max(Q[ind][tuple(state)]), 20)\n",
        "\n",
        "\n",
        "          prev_state = next_state\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10:\n",
        "            #print(\"Reached end of episode\")\n",
        "            #stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQC5l-kfU9MB"
      },
      "source": [
        "#multi_agent_qLearning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T_uSFq1MIwT"
      },
      "source": [
        "def MA_QL(env, num_episodes, discount_factor = 1.0,\n",
        "                            alpha = 0.6, epsilon = 0.1):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: np.zeros(env.action_space[ind].n)))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "    t_vals = []\n",
        "    non_zero_final = 0\n",
        "    final_rewards = []\n",
        "    for ith_episode in range(num_episodes):\n",
        "        #print(\"On episode: \", ith_episode)\n",
        "        states = env.reset()\n",
        "        for t in itertools.count():\n",
        "          #stores action probabilities assigned to each action at every step\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, p,state,agent) in enumerate(zip(Q, stats, policy, states, env.agents)):\n",
        "\n",
        "            # get probabilities of all actions from current state for each agent\n",
        "            action_probabilities = p(tuple(state))\n",
        "\n",
        "            action = np.random.choice(np.arange(\n",
        "                      len(action_probabilities)),\n",
        "                      p = action_probabilities)\n",
        "\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "              else:\n",
        "                action_probabilities[i] = 0\n",
        "\n",
        "            action_probs.append(action_probabilities)\n",
        "            actions.append(action)\n",
        "\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "          #print(reward)\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, p,state,agent) in enumerate(zip(Q, stats, policy, states, env.agents)):\n",
        "            # Update statistics\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "\n",
        "            #RUQL approximation algorithm, models Equation 4 from the paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "            best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "            td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "            td_delta = td_target - q[tuple(state)][action]\n",
        "            q[tuple(state)][action] += alpha * td_delta\n",
        "\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "            #print(\"Reached repeat_count update\")\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10: #or (ith_episode > 200 and t > 200):\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] != 0:\n",
        "              #print(\"Final reward was: \", reward[0])\n",
        "              non_zero_final += 1\n",
        "\n",
        "            #print(\"Finished episode: \", ith_episode)\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          states = next_state\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (num_episodes - non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYe1q4cJVEeh"
      },
      "source": [
        "#Multi_agent_RUQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-75nim60IbR"
      },
      "source": [
        "def RUQL(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        #stores action probabilities assigned to each action at every step\n",
        "        actions = []\n",
        "        action_probs = []\n",
        "\n",
        "        if ith_episode > 0:\n",
        "          alpha = (1000 + 1)/(1000 + ith_episode)\n",
        "\n",
        "        if (ith_episode / num_episodes > cutoff):\n",
        "          cutoff += .1\n",
        "          epsilon /= 2\n",
        "          env.world.agents[0].goal_b = np.random.choice(env.world.landmarks)\n",
        "\n",
        "        for ind, (q,stat, p,state,agent) in enumerate(zip(Q, stats, policy, prev_state, env.agents)):\n",
        "\n",
        "          #Create new policy function based on updated Q table for a given agent\n",
        "          policy[ind] = createEpsilonGreedyPolicy(q, epsilon, env.action_space[ind].n)\n",
        "          p = policy[ind]\n",
        "\n",
        "          #Get probability distribution over actions from policy\n",
        "          action_probabilities = p(tuple(state))\n",
        "\n",
        "          #Choose an action using the probability distribution over actions\n",
        "          action = np.random.choice(np.arange(\n",
        "                    len(action_probabilities)),\n",
        "                    p = action_probabilities)\n",
        "\n",
        "          #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "          for i in range(len(action_probabilities)):\n",
        "            if i == action:\n",
        "              action_probabilities[i] = 1\n",
        "            else:\n",
        "              action_probabilities[i] = 0\n",
        "\n",
        "          #action_probs -> stores one-hot encoded action for each agent\n",
        "          action_probs.append(action_probabilities)\n",
        "          #actions -> stores action chosen for each agent\n",
        "          actions.append(action)\n",
        "\n",
        "        #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "        next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "        #make rewards positive to allow policy function to choose argmin rather than argmax\n",
        "        reward = np.asarray(reward)\n",
        "        #next_state -> set of states for each agent\n",
        "        #reward -> array of rewards for each agent\n",
        "        #done -> whether or not learning is finished\n",
        "        #loops through each agent\n",
        "        for ind, (q,stat, p,state,agent) in enumerate(zip(Q, stats, policy, prev_state, env.agents)):\n",
        "\n",
        "          stat.episode_rewards[ith_episode] += reward[ind]\n",
        "          stat.episode_lengths[ith_episode] = 1\n",
        "\n",
        "          #choose action given probabilities for each agent\n",
        "          action = actions[ind]\n",
        "          if ind == 0:\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "              if action == 0:\n",
        "                communication_count[0] += 1\n",
        "              if action == 1:\n",
        "                communication_count[1] += 1\n",
        "              if action == 2:\n",
        "                communication_count[2] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, GREEN):\n",
        "              if action == 0:\n",
        "                communication_count[3] += 1\n",
        "              if action == 1:\n",
        "                communication_count[4] += 1\n",
        "              if action == 2:\n",
        "                communication_count[5] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "              if action == 0:\n",
        "                communication_count[6] += 1\n",
        "              if action == 1:\n",
        "                communication_count[7] += 1\n",
        "              if action == 2:\n",
        "                communication_count[8] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, CYAN):\n",
        "              if action == 0:\n",
        "                communication_count[9] += 1\n",
        "              if action == 1:\n",
        "                communication_count[10] += 1\n",
        "              if action == 2:\n",
        "                communication_count[11] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, MAGENTA):\n",
        "              if action == 0:\n",
        "                communication_count[12] += 1\n",
        "              if action == 1:\n",
        "                communication_count[13] += 1\n",
        "              if action == 2:\n",
        "                communication_count[14] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, YELLOW):\n",
        "              if action == 0:\n",
        "                communication_count[15] += 1\n",
        "              if action == 1:\n",
        "                communication_count[16] += 1\n",
        "              if action == 2:\n",
        "                communication_count[17] += 1\n",
        "\n",
        "\n",
        "          if ind == 1:\n",
        "            if q[tuple(state)].sum() == 0:\n",
        "              unseen += 1\n",
        "\n",
        "          # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "          #q -> q table for each agent\n",
        "          #state -> state for each agent\n",
        "          #action -> selected best action for each agent\n",
        "\n",
        "          # RUQL\n",
        "          left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "          right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "          best_next_action = np.argmin(q[tuple(next_state[ind])])\n",
        "          td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "          right_term *= td_target\n",
        "          q[tuple(state)][action] = left_term + right_term\n",
        "\n",
        "        prev_state = next_state\n",
        "\n",
        "    return Q, stats, unseen, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFRLCOVQV2Z8"
      },
      "source": [
        "#RUQL with Bandit-style UCB exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuubCPnV9nr"
      },
      "source": [
        "def RUQL_UCB_Bandit(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "            right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "            best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "            td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "            right_term *= td_target\n",
        "            q[tuple(state)][action] = left_term + right_term\n",
        "\n",
        "          prev_state = next_state\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10: #or (ith_episode > 200 and t > 200):\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] != 0:\n",
        "              #print(\"Final reward was: \", reward[0])\n",
        "              non_zero_final += 1\n",
        "\n",
        "            #print(\"Finished episode: \", ith_episode)\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (num_episodes - non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLYpXEUjgrsD"
      },
      "source": [
        "#RUQL vs. QL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZfEWzVgqjE"
      },
      "source": [
        "def RUQL_vs_QL(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "\n",
        "            action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            if ind == 1:\n",
        "              left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "              right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              right_term *= td_target\n",
        "              q[tuple(state)][action] = left_term + right_term\n",
        "            else:\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              td_delta = td_target - q[tuple(state)][action]\n",
        "              q[tuple(state)][action] += alpha * td_delta\n",
        "\n",
        "          prev_state = next_state\n",
        "          if repeat_count > 10:\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] == 0:\n",
        "              non_zero_final += 1\n",
        "\n",
        "            repeat_count = 0\n",
        "            break\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCgrhtN1E7qe"
      },
      "source": [
        "#RUQL vs. Oracle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vq35EdE_yV"
      },
      "source": [
        "def RUQL_vs_Oracle(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            if ind == 1:\n",
        "              action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "            else:\n",
        "              if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "                action = 0\n",
        "              elif np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "                action = 1\n",
        "              else:\n",
        "                action = 2\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            if ind == 1:\n",
        "              left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "              right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              right_term *= td_target\n",
        "              q[tuple(state)][action] = left_term + right_term\n",
        "            else:\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              td_delta = td_target - q[tuple(state)][action]\n",
        "              q[tuple(state)][action] += alpha * td_delta\n",
        "\n",
        "          prev_state = next_state\n",
        "          if repeat_count > 10:\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] == 0:\n",
        "              non_zero_final += 1\n",
        "\n",
        "            repeat_count = 0\n",
        "            break\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-BhhLBnd8YA"
      },
      "source": [
        "#EXP3-DH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rDzHYz-eCCk"
      },
      "source": [
        "def EXP3_DH(env, num_episodes, beta = 10, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    stats = []\n",
        "    seen_states = []\n",
        "    weights = []\n",
        "    probs = []\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)))\n",
        "\n",
        "      weights.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "\n",
        "      #calculate probabilities for each action\n",
        "      prob = []\n",
        "      for i in range(env.action_space[ind].n):\n",
        "        term = (1 - epsilon)\n",
        "        term *= np.exp(weights[ind][i])\n",
        "        term *= 1/(np.sum(np.exp(weights[ind])))\n",
        "        term += epsilon/env.action_space[ind].n\n",
        "        prob.append(term)\n",
        "\n",
        "      probs.append(prob)\n",
        "\n",
        "    # For every episode\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    cutoff = .2\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        #Update epsilon based on time step\n",
        "        if ith_episode > 0:\n",
        "          epsilon = ith_episode ** (-.2)\n",
        "\n",
        "        #stores action probabilities assigned to each action at every step\n",
        "        actions = []\n",
        "        action_probs = []\n",
        "\n",
        "        #For every cutoff % completion, change the color to ensure the speaker learns to change colors correctly\n",
        "        if (ith_episode / num_episodes > cutoff):\n",
        "          cutoff += .2\n",
        "          env.world.agents[0].goal_b = np.random.choice(env.world.landmarks)\n",
        "\n",
        "        for ind, (weight, stat, state, agent) in enumerate(zip(weights, stats, prev_state, env.agents)):\n",
        "\n",
        "          #Define K to be the number of actions for the current agent\n",
        "          K = env.action_space[ind].n\n",
        "\n",
        "          #Calculate probabilities for each action using equation defined in Step 4 of the algorithm\n",
        "          p = []\n",
        "          for i in range(K):\n",
        "            p_i = (1 - epsilon)\n",
        "            p_i *= np.exp(weight[i] - np.max(weight))\n",
        "            p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "            if p_i == float(\"inf\"):\n",
        "              print(\"Infinity weights\", weight)\n",
        "            p_i += epsilon/K\n",
        "            p.append(p_i)\n",
        "\n",
        "          probs[ind] = p\n",
        "\n",
        "          #Define action_probabilities to be the probability distribution over actions\n",
        "          action_probabilities = np.asarray(p)\n",
        "\n",
        "          #Choose an action using the probability distribution over actions\n",
        "          action = np.random.choice(np.arange(\n",
        "                    len(action_probabilities)),\n",
        "                    p = action_probabilities)\n",
        "\n",
        "\n",
        "          #print(action_probabilities)\n",
        "          #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "          for i in range(len(action_probabilities)):\n",
        "            if i == action:\n",
        "              action_probabilities[i] = 1\n",
        "            else:\n",
        "              action_probabilities[i] = 0\n",
        "\n",
        "          #action_probs -> stores one-hot encoded action for each agent\n",
        "          action_probs.append(action_probabilities)\n",
        "          #actions -> stores action chosen for each agent\n",
        "          actions.append(action)\n",
        "\n",
        "        #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "        next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "        #make rewards positive to allow policy function to choose argmin rather than argmax\n",
        "        #next_state -> set of states for each agent\n",
        "        #reward -> array of rewards for each agent\n",
        "        #done -> whether or not learning is finished\n",
        "        reward = np.asarray(reward)\n",
        "        #loops through each agent\n",
        "        for ind, (stat, prob, state, agent) in enumerate(zip(stats, probs, prev_state, env.agents)):\n",
        "\n",
        "          #Define K to be the number of actions for the current agent\n",
        "          K = env.action_space[ind].n\n",
        "\n",
        "          #Update statistics with rewards\n",
        "          stat.episode_rewards[ith_episode] += reward[ind]\n",
        "          stat.episode_lengths[ith_episode] = 1\n",
        "\n",
        "          #choose action given probabilities for each agent\n",
        "          action = actions[ind]\n",
        "\n",
        "\n",
        "          #Calculate payoff estimate for each action (0 for every non-chosen action)\n",
        "          payoff_estimate = []\n",
        "          for i in range(K):\n",
        "            if i == action:\n",
        "              payoff_estimate.append(reward[ind]/prob[i])\n",
        "            else:\n",
        "              payoff_estimate.append(0)\n",
        "\n",
        "          #Calculate weight for each action using Equation 8 in algorithm\n",
        "          weight = []\n",
        "          for i in range(K):\n",
        "            weight.append((ith_episode/(ith_episode + 1))**(beta)*weights[ind][i] + payoff_estimate[i])\n",
        "\n",
        "          weights[ind] = np.asarray(weight)\n",
        "\n",
        "\n",
        "          if ind == 0:\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "              if action == 0:\n",
        "                communication_count[0] += 1\n",
        "              if action == 1:\n",
        "                communication_count[1] += 1\n",
        "              if action == 2:\n",
        "                communication_count[2] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, GREEN):\n",
        "              if action == 0:\n",
        "                communication_count[3] += 1\n",
        "              if action == 1:\n",
        "                communication_count[4] += 1\n",
        "              if action == 2:\n",
        "                communication_count[5] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "              if action == 0:\n",
        "                communication_count[6] += 1\n",
        "              if action == 1:\n",
        "                communication_count[7] += 1\n",
        "              if action == 2:\n",
        "                communication_count[8] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, CYAN):\n",
        "              if action == 0:\n",
        "                communication_count[9] += 1\n",
        "              if action == 1:\n",
        "                communication_count[10] += 1\n",
        "              if action == 2:\n",
        "                communication_count[11] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, MAGENTA):\n",
        "              if action == 0:\n",
        "                communication_count[12] += 1\n",
        "              if action == 1:\n",
        "                communication_count[13] += 1\n",
        "              if action == 2:\n",
        "                communication_count[14] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, YELLOW):\n",
        "              if action == 0:\n",
        "                communication_count[15] += 1\n",
        "              if action == 1:\n",
        "                communication_count[16] += 1\n",
        "              if action == 2:\n",
        "                communication_count[17] += 1\n",
        "\n",
        "\n",
        "        prev_state = next_state\n",
        "\n",
        "    return stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OL226o0k31t"
      },
      "source": [
        "#RUQL vs. EXP3-DH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsOVRanIk8jI"
      },
      "source": [
        "def RUQL_vs_EXP3_DH(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01, beta = 10):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "      if ind == 0:\n",
        "        weight = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            if ind == 1:\n",
        "              action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "            else:\n",
        "              #Define K to be the number of actions for the current agent\n",
        "              K = env.action_space[ind].n\n",
        "\n",
        "              #Calculate probabilities for each action using equation defined in Step 4 of the algorithm\n",
        "              p = []\n",
        "              for i in range(K):\n",
        "                p_i = (1 - epsilon)\n",
        "                p_i *= np.exp(weight[i] - np.max(weight))\n",
        "                p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "                if p_i == float(\"inf\"):\n",
        "                  print(\"Infinity weights\", weight)\n",
        "                p_i += epsilon/K\n",
        "                p.append(p_i)\n",
        "\n",
        "              #Define action_probabilities to be the probability distribution over actions\n",
        "              action_probabilities = np.asarray(p)\n",
        "\n",
        "              #Choose an action using the probability distribution over actions\n",
        "              action = np.random.choice(np.arange(\n",
        "                        len(action_probabilities)),\n",
        "                        p = action_probabilities)\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            if ind == 1:\n",
        "              left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "              right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              right_term *= td_target\n",
        "              q[tuple(state)][action] = left_term + right_term\n",
        "            else:\n",
        "              payoff_estimate = []\n",
        "              for i in range(K):\n",
        "                if i == action:\n",
        "                  payoff_estimate.append(reward[ind]/p[i])\n",
        "                else:\n",
        "                  payoff_estimate.append(0)\n",
        "\n",
        "              #Calculate weight for each action using Equation 8 in algorithm\n",
        "              weights = []\n",
        "              for i in range(K):\n",
        "                weights.append((t/(t + 1))**(beta)*weight[i] + payoff_estimate[i])\n",
        "\n",
        "              weight = np.asarray(weights)\n",
        "              #print(weight)\n",
        "\n",
        "          prev_state = next_state\n",
        "          if repeat_count > 10: #or (ith_episode > 200 and t > 200):\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] == 0:\n",
        "              non_zero_final += 1\n",
        "\n",
        "            repeat_count = 0\n",
        "            break\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "        #print(\"Finished episode: \", ith_episode)\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLjo6q0PHMR4"
      },
      "source": [
        "#RUQL vs. EXP3-DH with 3 instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfunwlTJHPnw"
      },
      "source": [
        "def RUQL_vs_EXP3_DH_3(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01, beta = 10):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "      if ind == 0:\n",
        "        weights = np.asarray([np.zeros(env.action_space[ind].n), np.zeros(env.action_space[ind].n), np.zeros(env.action_space[ind].n)])\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            if ind == 1:\n",
        "              action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "            else:\n",
        "              #Define K to be the number of actions for the current agent\n",
        "              K = env.action_space[ind].n\n",
        "              if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "                weight = weights[0]\n",
        "              elif np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "                weight = weights[1]\n",
        "              else:\n",
        "                weight = weights[2]\n",
        "              #Calculate probabilities for each action using equation defined in Step 4 of the algorithm\n",
        "              p = []\n",
        "              for i in range(K):\n",
        "                p_i = (1 - epsilon)\n",
        "                p_i *= np.exp(weight[i] - np.max(weight))\n",
        "                p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "                if p_i == float(\"inf\"):\n",
        "                  print(\"Infinity weights\", weight)\n",
        "                p_i += epsilon/K\n",
        "                p.append(p_i)\n",
        "\n",
        "              #Define action_probabilities to be the probability distribution over actions\n",
        "              action_probabilities = np.asarray(p)\n",
        "\n",
        "              #Choose an action using the probability distribution over actions\n",
        "              action = np.random.choice(np.arange(\n",
        "                        len(action_probabilities)),\n",
        "                        p = action_probabilities)\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            if ind == 1:\n",
        "              left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "              right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "              best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "              td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "              right_term *= td_target\n",
        "              q[tuple(state)][action] = left_term + right_term\n",
        "            else:\n",
        "              payoff_estimate = []\n",
        "              for i in range(K):\n",
        "                if i == action:\n",
        "                  payoff_estimate.append(reward[ind]/p[i])\n",
        "                else:\n",
        "                  payoff_estimate.append(0)\n",
        "\n",
        "              #Calculate weight for each action using Equation 8 in algorithm\n",
        "              new_weights = []\n",
        "              for i in range(K):\n",
        "                new_weights.append((t/(t + 1))**(beta)*weight[i] + payoff_estimate[i])\n",
        "\n",
        "              if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "                weights[0] = np.asarray(new_weights)\n",
        "              elif np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "                weights[1] = np.asarray(new_weights)\n",
        "              else:\n",
        "                weights[2] = np.asarray(new_weights)\n",
        "              #print(weight)\n",
        "\n",
        "          prev_state = next_state\n",
        "          if repeat_count > 10: #or (ith_episode > 200 and t > 200):\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] == 0:\n",
        "              non_zero_final += 1\n",
        "            repeat_count = 0\n",
        "            break\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "        #print(\"Finished episode: \", ith_episode)\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhNMedE8QOct"
      },
      "source": [
        "#EXP3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZO3np9LQQjo"
      },
      "source": [
        "def EXP3(env, num_episodes, beta = 10, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    stats = []\n",
        "    seen_states = []\n",
        "    weights = []\n",
        "    probs = []\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)))\n",
        "\n",
        "      weights.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "\n",
        "      #calculate probabilities for each action\n",
        "      prob = []\n",
        "      for i in range(env.action_space[ind].n):\n",
        "        term = (1 - epsilon)\n",
        "        term *= np.exp(weights[ind][i])\n",
        "        term *= 1/(np.sum(np.exp(weights[ind])))\n",
        "        term += epsilon/env.action_space[ind].n\n",
        "        prob.append(term)\n",
        "\n",
        "      probs.append(prob)\n",
        "\n",
        "    # For every episode\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    cutoff = .2\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        #Update epsilon based on time step\n",
        "        if ith_episode > 0:\n",
        "          epsilon = ith_episode ** (-.2)\n",
        "\n",
        "        #stores action probabilities assigned to each action at every step\n",
        "        actions = []\n",
        "        action_probs = []\n",
        "\n",
        "        #For every cutoff % completion, change the color to ensure the speaker learns to change colors correctly\n",
        "        if (ith_episode / num_episodes > cutoff):\n",
        "          cutoff += .2\n",
        "          env.world.agents[0].goal_b = np.random.choice(env.world.landmarks)\n",
        "\n",
        "        for ind, (weight, stat, state, agent) in enumerate(zip(weights, stats, prev_state, env.agents)):\n",
        "\n",
        "          #Define K to be the number of actions for the current agent\n",
        "          K = env.action_space[ind].n\n",
        "\n",
        "          #Calculate probabilities for each action using equation defined in Step 4 of the algorithm\n",
        "          p = []\n",
        "          for i in range(K):\n",
        "            p_i = (1 - epsilon)\n",
        "            p_i *= np.exp(weight[i] - np.max(weight))\n",
        "            p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "            if p_i == float(\"inf\"):\n",
        "              print(\"Infinity weights\", weight)\n",
        "            p_i += epsilon/K\n",
        "            p.append(p_i)\n",
        "\n",
        "          probs[ind] = p\n",
        "\n",
        "          #Define action_probabilities to be the probability distribution over actions\n",
        "          action_probabilities = np.asarray(p)\n",
        "\n",
        "          #Choose an action using the probability distribution over actions\n",
        "          action = np.random.choice(np.arange(\n",
        "                    len(action_probabilities)),\n",
        "                    p = action_probabilities)\n",
        "\n",
        "\n",
        "          #print(action_probabilities)\n",
        "          #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "          for i in range(len(action_probabilities)):\n",
        "            if i == action:\n",
        "              action_probabilities[i] = 1\n",
        "            else:\n",
        "              action_probabilities[i] = 0\n",
        "\n",
        "          #action_probs -> stores one-hot encoded action for each agent\n",
        "          action_probs.append(action_probabilities)\n",
        "          #actions -> stores action chosen for each agent\n",
        "          actions.append(action)\n",
        "\n",
        "        #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "        next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "        #make rewards positive to allow policy function to choose argmin rather than argmax\n",
        "        #next_state -> set of states for each agent\n",
        "        #reward -> array of rewards for each agent\n",
        "        #done -> whether or not learning is finished\n",
        "        reward = np.asarray(reward)\n",
        "        #loops through each agent\n",
        "        for ind, (stat, prob, state, agent) in enumerate(zip(stats, probs, prev_state, env.agents)):\n",
        "\n",
        "          #Define K to be the number of actions for the current agent\n",
        "          K = env.action_space[ind].n\n",
        "\n",
        "          #Update statistics with rewards\n",
        "          stat.episode_rewards[ith_episode] += reward[ind]\n",
        "          stat.episode_lengths[ith_episode] = 1\n",
        "\n",
        "          #choose action given probabilities for each agent\n",
        "          action = actions[ind]\n",
        "\n",
        "\n",
        "          #Calculate payoff estimate for each action (0 for every non-chosen action)\n",
        "          payoff_estimate = []\n",
        "          for i in range(K):\n",
        "            if i == action:\n",
        "              payoff_estimate.append(reward[ind]/prob[i])\n",
        "            else:\n",
        "              payoff_estimate.append(0)\n",
        "\n",
        "          #Calculate weight for each action using Equation 8 in algorithm\n",
        "\n",
        "          weights[ind] = np.asarray(weights[ind])*np.exp(epsilon*np.asarray(payoff_estimate)/K)\n",
        "\n",
        "\n",
        "          if ind == 0:\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, RED):\n",
        "              if action == 0:\n",
        "                communication_count[0] += 1\n",
        "              if action == 1:\n",
        "                communication_count[1] += 1\n",
        "              if action == 2:\n",
        "                communication_count[2] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, GREEN):\n",
        "              if action == 0:\n",
        "                communication_count[3] += 1\n",
        "              if action == 1:\n",
        "                communication_count[4] += 1\n",
        "              if action == 2:\n",
        "                communication_count[5] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, BLUE):\n",
        "              if action == 0:\n",
        "                communication_count[6] += 1\n",
        "              if action == 1:\n",
        "                communication_count[7] += 1\n",
        "              if action == 2:\n",
        "                communication_count[8] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, CYAN):\n",
        "              if action == 0:\n",
        "                communication_count[9] += 1\n",
        "              if action == 1:\n",
        "                communication_count[10] += 1\n",
        "              if action == 2:\n",
        "                communication_count[11] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, MAGENTA):\n",
        "              if action == 0:\n",
        "                communication_count[12] += 1\n",
        "              if action == 1:\n",
        "                communication_count[13] += 1\n",
        "              if action == 2:\n",
        "                communication_count[14] += 1\n",
        "            if np.array_equal(env.world.agents[0].goal_b.color, YELLOW):\n",
        "              if action == 0:\n",
        "                communication_count[15] += 1\n",
        "              if action == 1:\n",
        "                communication_count[16] += 1\n",
        "              if action == 2:\n",
        "                communication_count[17] += 1\n",
        "\n",
        "\n",
        "        prev_state = next_state\n",
        "\n",
        "    return stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmGJHYgHhLQS"
      },
      "source": [
        "#V-OL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ZOG8ErhNE_"
      },
      "source": [
        "def V_OL(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01, beta = 10, eta = .01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    non_zero_final = 0\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    H = 200\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (200)))\n",
        "\n",
        "      N.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      if ind == 0:\n",
        "        weight = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append((1/env.action_space[ind].n)*np.ones(env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "    final_rewards = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            iota = np.log((121*env.action_space[0].n*num_episodes*t)/.1)\n",
        "            alpha = (H + 1)/(H + t)\n",
        "            beta = np.sqrt((H**4*env.action_space[0].n*iota)/t)\n",
        "            eta = np.sqrt(np.log(env.action_space[0].n)/(env.action_space[0].n*t))\n",
        "\n",
        "          for ind, (q,p,stat, n, v, state,agent) in enumerate(zip(Q, policy,stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            #Define action_probabilities to be the probability distribution over actions\n",
        "            action_probabilities = np.asarray(p)\n",
        "            print(action_probabilities)\n",
        "\n",
        "            #Choose an action using the probability distribution over actions\n",
        "            action = np.random.choice(np.arange(\n",
        "                      len(action_probabilities)),\n",
        "                      p = action_probabilities)\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          #reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q, p, stat, state,agent) in enumerate(zip(Q, policy, stats, prev_state, env.agents)):\n",
        "\n",
        "\n",
        "            stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            #stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "            N[ind][tuple(state)] += 1\n",
        "            V[ind][tuple(state)] = (1-alpha)*V[ind][tuple(state)] + alpha*(reward[ind] + V[ind][tuple(next_state[ind])] + beta)\n",
        "            for i in range(env.action_space[ind].n):\n",
        "              if action == i:\n",
        "                Q[ind][tuple(state)][i] = (1 - alpha)*Q[ind][tuple(state)][i] + alpha*(200 - reward[ind] - V[ind][tuple(next_state[ind])])/(policy[ind][action] + eta)\n",
        "              else:\n",
        "                Q[ind][tuple(state)][i] = (1 - alpha)*Q[ind][tuple(state)][i]\n",
        "\n",
        "            for i in range(env.action_space[ind].n):\n",
        "              policy[ind][i] = (np.exp(-eta*Q[ind][tuple(state)][i]/alpha))/np.sum(np.exp(-eta*Q[ind][tuple(state)]/alpha))\n",
        "              print(\"V-value for action \", i, \"and agent \", ind, \": \", Q[ind][tuple(state)][i])\n",
        "              print(\"Policy for action \", i, \" and agent \", ind, \": \", policy[ind][i])\n",
        "\n",
        "          prev_state = next_state\n",
        "          if repeat_count > 10: #or (ith_episode > 200 and t > 200):\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            t_vals.append(t)\n",
        "            final_rewards.append(reward[0])\n",
        "            if reward[0] == 0:\n",
        "              non_zero_final += 1\n",
        "\n",
        "            repeat_count = 0\n",
        "            break\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "        #print(\"Finished episode: \", ith_episode)\n",
        "\n",
        "    print(\"Average H value: \", np.average(np.asarray(t_vals)))\n",
        "    print(\"Average final reward: \", np.average(np.asarray(final_rewards)))\n",
        "    print(\"Convergence rate: \", (non_zero_final)/(num_episodes))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9nQi303WFh6"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDyZ_L_16pOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b8cf35a1-decd-4c3d-98a9-41e0ba6e6147"
      },
      "source": [
        "e = 1000\n",
        "#MA_QL_Q, MA_QL_stats = MA_QL(env, e)\n",
        "#V_OL_Q, V_OL_stats = V_OL(env, e)\n",
        "RUQL_UCB_BANDIT_Q, RUQL_UCB_BANDIT_stats, RUQL_UCB_BANDIT_comm = RUQL_UCB_Bandit(env, e)\n",
        "#UCB_Hoeffding_Q, UCB_Hoeffding_stats, UCB_Hoeffding_comm = UCB_Hoeffding(env, e)\n",
        "#RUQL_vs_QL_Q, RUQL_vs_QL_stats, RUQL_vs_QL_comm = RUQL_vs_QL(env, e)\n",
        "#RUQL_vs_Oracle_Q, RUQL_vs_Oracle_stats, RUQL_vs_Oracle_comm = RUQL_vs_Oracle(env, e)\n",
        "#RUQL_vs_EXP3_DH_Q, RUQL_vs_EXP3_DH_stats, RUQL_vs_EXP3_DH_comm = RUQL_vs_EXP3_DH(env, e)\n",
        "#RUQL_vs_EXP3_DH_3_Q, RUQL_vs_EXP3_DH_3_stats, RUQL_vs_EXP3_DH_3_comm = RUQL_vs_EXP3_DH_3(env, e)\n",
        "#RUQL_UCB_Hoeffding_Q, RUQL_UCB_Hoeffding_stats, RUQL_UCB_Hoeffding_comm = RUQL_UCB_Hoeffding(env, e)\n",
        "#UCB_Microsoft_Q, UCB_Microsoft_stats, UCB_Microsoft_comm = UCB_Microsoft(env, e)\n",
        "#print(RUQL_UCB_Hoeffding_comm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6d22d2e79693>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#MA_QL_Q, MA_QL_stats = MA_QL(env, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#V_OL_Q, V_OL_stats = V_OL(env, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mRUQL_UCB_BANDIT_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRUQL_UCB_BANDIT_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRUQL_UCB_BANDIT_comm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUQL_UCB_Bandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#UCB_Hoeffding_Q, UCB_Hoeffding_stats, UCB_Hoeffding_comm = UCB_Hoeffding(env, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#RUQL_vs_QL_Q, RUQL_vs_QL_stats, RUQL_vs_QL_comm = RUQL_vs_QL(env, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-61a1e6ab69dc>\u001b[0m in \u001b[0;36mRUQL_UCB_Bandit\u001b[0;34m(env, num_episodes, discount_factor, alpha, epsilon)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m               \u001b[0mcommunication_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm_count_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunication_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGREEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCYAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAGENTA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYELLOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b90fe540722d>\u001b[0m in \u001b[0;36mcomm_count_update\u001b[0;34m(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomm_count_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunication_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGREEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCYAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAGENTA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYELLOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mcommunication_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_array_equal_dispatcher\u001b[0;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[1;32m   2372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_array_equal_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVZc-sAclnT6"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(RUQL_UCB_BANDIT_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(RUQL_UCB_BANDIT_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoBDDLmFaxES"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(UCB_Hoeffding_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(UCB_Hoeffding_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xig2QHT27gIM"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(MA_QL_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(MA_QL_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oko1GRJDkzLp"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(RUQL_vs_QL_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(RUQL_vs_QL_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1v1aroBlnE"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(UCB_Microsoft_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(UCB_Microsoft_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbAiWSVHrIC-"
      },
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "f = plt.figure()\n",
        "f.set_figwidth(10)\n",
        "f.set_figheight(10)\n",
        "locs = [4*i for i in range(9)]\n",
        "plt.bar(locs, RUQL_UCB_comm, color=['red', 'red', 'red', 'green', 'green', 'green', 'blue', 'blue', 'blue'], width=[3.75 for _ in range(9)])#, 'cyan', 'cyan', 'cyan', 'magenta', 'magenta', 'magenta', 'yellow', 'yellow', 'yellow'], width=[3.75 for _ in range(9)])\n",
        "plt.xticks(locs, ['red+0', 'red+1', 'red+2', 'green+0', 'green+1', 'green+2', 'blue+0', 'blue+1', 'blue+2'])#,'cyan+0', 'cyan+1', 'cyan+2', 'magenta+0', 'magenta+1', 'magenta+2', 'yellow+0', 'yellow+1', 'yellow+2'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6sr1VJ60c6b"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(RUQL_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(RUQL_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq6CdUNOHhJu"
      },
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "f = plt.figure()\n",
        "f.set_figwidth(10)\n",
        "f.set_figheight(10)\n",
        "locs = [4*i for i in range(9)]\n",
        "plt.bar(locs, RUQL_comm, color=['red', 'red', 'red', 'green', 'green', 'green', 'blue', 'blue', 'blue'], width=[3.75 for _ in range(9)])#, 'cyan', 'cyan', 'cyan', 'magenta', 'magenta', 'magenta', 'yellow', 'yellow', 'yellow'], width=[3.75 for _ in range(9)])\n",
        "plt.xticks(locs, ['red+0', 'red+1', 'red+2', 'green+0', 'green+1', 'green+2', 'blue+0', 'blue+1', 'blue+2'])#,'cyan+0', 'cyan+1', 'cyan+2', 'magenta+0', 'magenta+1', 'magenta+2', 'yellow+0', 'yellow+1', 'yellow+2'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEoReDd-QDuJ"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(EXP3_DH_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(EXP3_DH_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCHWMAtJRbVq"
      },
      "source": [
        "import plotting_custom\n",
        "for ind, stat in enumerate(EXP3_stats):\n",
        "\n",
        "  print(\"Stats for agent {}\".format(ind))\n",
        "  print(f\"Average reward for agent {ind}: \" + str(np.mean(EXP3_stats[ind].episode_rewards)))\n",
        "  plotting_custom.plot_episode_stats(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zis7dL-p838x"
      },
      "source": [
        "#MADDPG Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeUzsHyKt8rJ",
        "outputId": "5bcce78c-f899-46a2-ca03-2dfc45eea7c0"
      },
      "source": [
        "%cd ../\n",
        "!git clone https://github.com/lab-sigma/cooperative-ML.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'cooperative-ML'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 67 (delta 22), reused 57 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (67/67), 454.77 KiB | 7.98 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znM-lwoyuYkr",
        "outputId": "0119bc0f-c944-4430-cb81-c7eb99828d4b"
      },
      "source": [
        "%cd ./cooperative-ML/multiagent-particle-envs/maddpg/experiments"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cooperative-ML/multiagent-particle-envs/maddpg/experiments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AF-ZMDRugWl",
        "outputId": "06f15d4e-5e0d-4322-a28a-9f9b676a110b"
      },
      "source": [
        "!pip install tensorflow==2.15.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tensorboard-data-server, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sONG9CwG2SEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411c1590-e3ce-457e-d2fe-754c63e7b5b0"
      },
      "source": [
        "!python train.py --scenario simple_speaker_listener --num-episodes 5000 --exp-name simple_speaker_listener --max-episode-len 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-18 05:31:13.794210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-18 05:31:13.794311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-18 05:31:13.796451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-18 05:31:13.839748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-18 05:31:15.356956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/cooperative-ML/multiagent-particle-envs/maddpg/experiments/train.py\", line 9, in <module>\n",
            "    import tensorflow.contrib.layers as layers\n",
            "ModuleNotFoundError: No module named 'tensorflow.contrib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tePBTVk86b2"
      },
      "source": [
        "#Centralized Training Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cHQJGK19Ciw"
      },
      "source": [
        "def RUQL_centralized(env, num_episodes, discount_factor = 0.9,\n",
        "                            alpha = 0.01, epsilon = 0.01):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    V = []\n",
        "    N = []\n",
        "    stats = []\n",
        "    policy = []\n",
        "    seen_states = []\n",
        "    unseen = 0\n",
        "    #Loop through each agent in the environment\n",
        "    for ind, agent in enumerate(env.agents):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: (np.zeros(env.action_space[ind].n))))\n",
        "      V.append(defaultdict(lambda: (0)))\n",
        "\n",
        "      N.append(np.zeros(env.action_space[ind].n))\n",
        "\n",
        "      # Keeps track of useful statistics for each agent\n",
        "      stats.append( EpisodeStats(\n",
        "          episode_lengths = np.zeros(num_episodes),\n",
        "          episode_rewards = np.zeros(num_episodes)) )\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[ind], epsilon, env.action_space[ind].n))\n",
        "\n",
        "    # For every episode\n",
        "    cutoff = .1\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    # red+0, red+1, red+2, green+0, green+1, green+2, blue+0, blue+1, blue+2,\n",
        "    # cyan+0, cyan+1, cyan+2, magenta+0, magenta+1, magenta+2, yellow+0, yellow+1, yellow+2\n",
        "    communication_count = [0 for _ in range(9)]\n",
        "    RED = np.array([0.65, 0.15, 0.15])\n",
        "    GREEN = np.array([0.15, 0.65, 0.15])\n",
        "    BLUE = np.array([0.15, 0.15, 0.65])\n",
        "    CYAN = np.array([0.15, 0.65, 0.65])\n",
        "    MAGENTA = np.array([0.65, 0.15, 0.65])\n",
        "    YELLOW = np.array([0.65, 0.65, 0.15])\n",
        "\n",
        "    prev_reward = 0\n",
        "    repeat_count = 0\n",
        "\n",
        "    t_vals = []\n",
        "\n",
        "    for ith_episode in range(num_episodes):\n",
        "\n",
        "        prev_state = env.reset()\n",
        "\n",
        "        for t in itertools.count():\n",
        "          actions = []\n",
        "          action_probs = []\n",
        "\n",
        "          if t > 0:\n",
        "            alpha = (200 + 1)/(200 + t)\n",
        "\n",
        "          #if (ith_episode / num_episodes > cutoff):\n",
        "          #  cutoff += .1\n",
        "          #  epsilon /= 2\n",
        "            #env.world.agents[0].goal_b = np.random.choice(env.world.landmarks)\n",
        "\n",
        "          for ind, (q,stat, n, v, state,agent) in enumerate(zip(Q, stats, N, V, prev_state, env.agents)):\n",
        "\n",
        "            #Choose an action using the UCB policy\n",
        "            action = UCBPolicy(q, n, 2, v[tuple(state)], tuple(state))\n",
        "\n",
        "            action_probabilities = np.zeros(env.action_space[ind].n)\n",
        "\n",
        "            #One-hot encode the chosen action so it is definitely chosen in env.step()\n",
        "            for i in range(len(action_probabilities)):\n",
        "              if i == action:\n",
        "                action_probabilities[i] = 1\n",
        "\n",
        "            #action_probs -> stores one-hot encoded action for each agent\n",
        "            action_probs.append(action_probabilities)\n",
        "            #actions -> stores action chosen for each agent\n",
        "            actions.append(action)\n",
        "\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          next_state, reward, done, _ = env.step(action_probs)\n",
        "\n",
        "          full_state = np.concatenate(prev_state[0], prev_state[1])\n",
        "\n",
        "          reward = (np.asarray(reward)/242)+1\n",
        "          #next_state -> set of states for each agent\n",
        "          #reward -> array of rewards for each agent\n",
        "          #done -> whether or not learning is finished\n",
        "          #loops through each agent\n",
        "          for ind, (q,stat, state,agent) in enumerate(zip(Q, stats, prev_state, env.agents)):\n",
        "\n",
        "            #stat.episode_rewards[ith_episode] += reward[ind]\n",
        "            stat.episode_rewards[ith_episode] = reward[ind]\n",
        "            stat.episode_lengths[ith_episode] = t\n",
        "\n",
        "            #choose action given probabilities for each agent\n",
        "            action = actions[ind]\n",
        "            if ind == 0:\n",
        "              communication_count = comm_count_update(env, action, communication_count, RED, GREEN, BLUE, CYAN, MAGENTA, YELLOW)\n",
        "\n",
        "            if ind == 1:\n",
        "              if q[tuple(state)].sum() == 0:\n",
        "                unseen += 1\n",
        "\n",
        "            # RUQL approximation algorithm, models Equation 4 from the RUQL paper\n",
        "            #q -> q table for each agent\n",
        "            #state -> state for each agent\n",
        "            #action -> selected best action for each agent\n",
        "\n",
        "            # RUQL\n",
        "            left_term = (1-alpha)**(1/(action_probs[ind][action]))*q[tuple(state)][action]\n",
        "            right_term = 1 - (1-alpha)**(1/(action_probs[ind][action]))\n",
        "            best_next_action = np.argmax(q[tuple(next_state[ind])])\n",
        "            td_target = reward[ind] + discount_factor * q[tuple(next_state[ind])][best_next_action]\n",
        "            right_term *= td_target\n",
        "            q[tuple(state)][action] = left_term + right_term\n",
        "\n",
        "          prev_state = next_state\n",
        "          if prev_reward == reward[0]:\n",
        "            repeat_count += 1\n",
        "          else:\n",
        "            repeat_count = 0\n",
        "          if repeat_count > 10:\n",
        "            #print(\"Reached end of episode\")\n",
        "            #stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]/t\n",
        "            stat.episode_rewards[ith_episode] = stat.episode_rewards[ith_episode]\n",
        "            repeat_count = 0\n",
        "            break\n",
        "\n",
        "          prev_reward = reward[0]\n",
        "\n",
        "    print(\"Average H value\", np.average(np.asarray(t_vals)))\n",
        "\n",
        "    return Q, stats, communication_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U0IbCyAF6R-"
      },
      "source": [
        "#Bayesian Persuasion Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQVbAl_L9QjH",
        "outputId": "e3fe934b-92cb-4285-8f26-5c7362df2cdf"
      },
      "source": [
        "#np.set_printoptions(suppress=False)\n",
        "np.set_printoptions(suppress=True)\n",
        "import cvxpy as cp\n",
        "num_states = 2\n",
        "num_signals = num_actions = 2\n",
        "\n",
        "state_prior = np.asarray([0.3, 0.7])  ### state { guilty, innocent}\n",
        "\n",
        "s = np.asarray([[1,1], [0,0]]) ### state x action\n",
        "r = np.asarray([[1,0], [0,1]])\n",
        "\n",
        "\n",
        "x = cp.Variable( num_states*num_signals )\n",
        "s_flat = np.reshape(s, num_states*num_signals )\n",
        "obj = cp.Maximize( cp.matmul(x, s_flat)  )\n",
        "\n",
        "P = cp.reshape(x, (num_states, num_signals) )\n",
        "p1 = cp.sum(P, axis=1)\n",
        "constraints =  [p1 == state_prior, x >= 0]\n",
        "\n",
        "for i in range(num_actions):\n",
        "\tfor j in range(num_actions):\n",
        "\t\tconstraints.append( cp.matmul(P[:,i], r[:,i]) >=  cp.matmul(P[:,i], r[:,j]) )\n",
        "\n",
        "lp = cp.Problem(obj, constraints)\n",
        "lp.solve()\n",
        "\n",
        "print(\"LP results: ==============\\n\")\n",
        "print(\"status: \" + str(lp.status) + \"\\n\")\n",
        "print(P.value.T / state_prior)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP results: ==============\n",
            "\n",
            "status: optimal\n",
            "\n",
            "[[ 1.          0.42857143]\n",
            " [-0.          0.57142857]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICQV9Sm06JpY"
      },
      "source": [
        "prior_dist = np.asarray([.7, .3])\n",
        "state = np.random.choice([0, 1], p=prior_dist)\n",
        "\n",
        "def reward(state, choice):\n",
        "  if state == 0 and choice == 0:\n",
        "    return [1, 1]\n",
        "  elif state == 1 and choice == 0:\n",
        "    return [1, 0]\n",
        "  elif state == 0 and choice == 1:\n",
        "    return [0, 0]\n",
        "  elif state == 1 and choice == 1:\n",
        "    return [0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VAsYNBjWoNP",
        "outputId": "31df9ab4-5f70-4ebe-cb61-b3c96399011c"
      },
      "source": [
        "zero_zero_probs = []\n",
        "zero_one_probs = []\n",
        "one_zero_probs = []\n",
        "one_one_probs = []\n",
        "def BP_EXP3(num_rounds, prior_dist, discount_factor = .6, alpha = 0.6, epsilon = 0.1, beta = 4):\n",
        "  weights = [[], []]\n",
        "  for i in range(2):\n",
        "    weights[i] = [np.asarray([0, 0]), np.asarray([0, 0])]\n",
        "\n",
        "  states = [np.random.choice([0, 1], p=prior_dist), 0]\n",
        "  next_states = states\n",
        "  index = 0\n",
        "  actions = [0, 0]\n",
        "  zero_rounds = 0\n",
        "  truthful_zero_rounds = 0\n",
        "  one_rounds = 0\n",
        "  truthful_one_rounds = 0\n",
        "  K = 2\n",
        "  for t in range(num_rounds):\n",
        "    if t % 2 == 0:\n",
        "      index = 0\n",
        "    else:\n",
        "      index = 1\n",
        "      states[1] = actions[0]\n",
        "\n",
        "    #if t > 0:\n",
        "    #  epsilon = t ** (-.2)\n",
        "\n",
        "    state = states[index]\n",
        "    weight = weights[index][state]\n",
        "    #print(\"Weights: \", weights)\n",
        "    if index == 0:\n",
        "      p_vals = [[], []]\n",
        "    p = []\n",
        "\n",
        "    for i in range(K):\n",
        "      #print(\"Weight for action \", i, \" for agent \", index, \": \", weight[i])\n",
        "      p_i = (1 - epsilon)\n",
        "      p_i *= np.exp(weight[i] - np.max(weight))\n",
        "      p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "      #print(\"p_i: \", p_i)\n",
        "      p_i += epsilon/K\n",
        "      p.append(p_i)\n",
        "\n",
        "    action_probabilities = np.asarray(p)\n",
        "\n",
        "    p_vals[index] = action_probabilities\n",
        "\n",
        "    action = np.random.choice(np.arange(\n",
        "            len(action_probabilities)),\n",
        "            p = action_probabilities)\n",
        "\n",
        "    if index == 0:\n",
        "      #print(\"Prosecutor sent signal \", action, \" on episode \", t/2, \" with state of nature: \", state)\n",
        "      if state == 0:\n",
        "        zero_rounds += 1\n",
        "        if action == 0:\n",
        "          truthful_zero_rounds += 1\n",
        "        zero_zero_probs.append(p[0])\n",
        "        zero_one_probs.append(p[1])\n",
        "      else:\n",
        "        one_rounds += 1\n",
        "        if action == 1:\n",
        "          truthful_one_rounds += 1\n",
        "        one_zero_probs.append(p[0])\n",
        "        one_one_probs.append(p[1])\n",
        "\n",
        "    actions[index] = action\n",
        "\n",
        "    if index == 1:\n",
        "      #print(\"State: \", states[0])\n",
        "      #if states[0] == 0:\n",
        "\n",
        "      other_p = []\n",
        "      other_state = int(not states[0])\n",
        "      other_weight = weights[index][state]\n",
        "      for i in range(K):\n",
        "        #print(\"Weight for action \", i, \" for agent \", index, \": \", other_weight[i])\n",
        "        p_i = (1 - epsilon)\n",
        "        p_i *= np.exp(other_weight[i] - np.max(other_weight))\n",
        "        p_i *= 1/(np.sum(np.exp(other_weight - np.max(other_weight))))\n",
        "        #print(\"p_i: \", p_i)\n",
        "        p_i += epsilon/K\n",
        "        other_p.append(p_i)\n",
        "\n",
        "      sums = []\n",
        "      for i in range(2):\n",
        "        pi = p_vals[0]\n",
        "        p_theta_1 = pi[actions[0]]*prior_dist[states[0]]/(prior_dist[states[0]]*p_vals[index][states[1]] + prior_dist[other_state]*other_p[states[1]])\n",
        "        #print(\"p_theta_1: \", p_theta_1)\n",
        "        r_1 = reward(states[0], i)[1]\n",
        "        p_theta_2 = pi[actions[0]]*prior_dist[other_state]/(prior_dist[other_state]*p_vals[index][states[1]] + prior_dist[states[0]]*other_p[states[1]])\n",
        "        r_2 = reward(other_state, i)[1]\n",
        "        sum = p_theta_1*r_1 + p_theta_2*r_2\n",
        "        sums.append(sum)\n",
        "\n",
        "      actions[1] = sums.index(max(sums))\n",
        "\n",
        "\n",
        "      #print(\"Action taken by sender: \", states[1])\n",
        "      #print(\"Action probabilities for the sender: \", p_vals[0], \" for state \", states[0])\n",
        "      #print(\"Action probabilities for the receiver: \", p_vals[1], \" for state \", states[0])\n",
        "\n",
        "      rewards = reward(states[0], actions[1])\n",
        "\n",
        "      payoff_estimates = [[], []]\n",
        "\n",
        "      for agent in range(2):\n",
        "        for i in range(K):\n",
        "          if i == actions[agent]:\n",
        "            payoff_estimates[agent].append(rewards[agent]/p_vals[agent][i])\n",
        "          else:\n",
        "            payoff_estimates[agent].append(0)\n",
        "\n",
        "      #print(\"Payoff estimates: \", payoff_estimates)\n",
        "\n",
        "      new_weights = [[], []]\n",
        "\n",
        "      for agent in range(2):\n",
        "        for i in range(K):\n",
        "          new_weights[agent].append((t/(t + 1))**(beta)*weights[agent][states[agent]][i] + payoff_estimates[agent][i])\n",
        "        weights[agent][states[agent]] = new_weights[agent]\n",
        "\n",
        "      states = [np.random.choice([0, 1], p=prior_dist), 0]\n",
        "\n",
        "  print(\"Weights: \", weights)\n",
        "  weight = weights[0][0]\n",
        "  p = []\n",
        "  for i in range(K):\n",
        "    #print(\"Weight for action \", i, \" for agent \", index, \": \", weight[i])\n",
        "    p_i = (1 - epsilon)\n",
        "    p_i *= np.exp(weight[i] - np.max(weight))\n",
        "    p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "    #print(\"p_i: \", p_i)\n",
        "    p_i += epsilon/K\n",
        "    p.append(p_i)\n",
        "  print(\"Prosecutor probabilities if client is guilty: \", p)\n",
        "  weight = weights[0][1]\n",
        "  p = []\n",
        "  for i in range(K):\n",
        "    #print(\"Weight for action \", i, \" for agent \", index, \": \", weight[i])\n",
        "    p_i = (1 - epsilon)\n",
        "    p_i *= np.exp(weight[i] - np.max(weight))\n",
        "    p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "    #print(\"p_i: \", p_i)\n",
        "    p_i += epsilon/K\n",
        "    p.append(p_i)\n",
        "  print(\"Prosecutor probabilities if client is innocent: \", p)\n",
        "  weight = weights[1][0]\n",
        "  p = []\n",
        "  for i in range(K):\n",
        "    #print(\"Weight for action \", i, \" for agent \", index, \": \", weight[i])\n",
        "    p_i = (1 - epsilon)\n",
        "    p_i *= np.exp(weight[i] - np.max(weight))\n",
        "    p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "    #print(\"p_i: \", p_i)\n",
        "    p_i += epsilon/K\n",
        "    p.append(p_i)\n",
        "  print(\"Judge probabilities if signal is 0: \", p)\n",
        "  weight = weights[1][1]\n",
        "  p = []\n",
        "  for i in range(K):\n",
        "    #print(\"Weight for action \", i, \" for agent \", index, \": \", weight[i])\n",
        "    p_i = (1 - epsilon)\n",
        "    p_i *= np.exp(weight[i] - np.max(weight))\n",
        "    p_i *= 1/(np.sum(np.exp(weight - np.max(weight))))\n",
        "    #print(\"p_i: \", p_i)\n",
        "    p_i += epsilon/K\n",
        "    p.append(p_i)\n",
        "  print(\"Judge probabilities if signal is 1: \", p)\n",
        "  if zero_rounds > 0:\n",
        "    print(\"\\n\\nProsecutor truthfully reported guilty with a ratio of: \", truthful_zero_rounds/zero_rounds)\n",
        "  if one_rounds > 0:\n",
        "    print(\"Prosecutor truthfully reported innocent with a ratio of: \", truthful_one_rounds/one_rounds)\n",
        "\n",
        "BP_EXP3(1000, prior_dist)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights:  [[[104.62153228199773, 146.11910783018138], [121.15146918982292, 130.73071586061295]], [[31.483860024229003, 0.0], [121.03862315033075, 0.0]]]\n",
            "Prosecutor probabilities if client is guilty:  [0.05, 0.9500000000000001]\n",
            "Prosecutor probabilities if client is innocent:  [0.0500622298149473, 0.9499377701850529]\n",
            "Judge probabilities if signal is 0:  [0.9499999999999809, 0.0500000000000191]\n",
            "Judge probabilities if signal is 1:  [0.9500000000000001, 0.05]\n",
            "\n",
            "\n",
            "Prosecutor truthfully reported guilty with a ratio of:  0.23372781065088757\n",
            "Prosecutor truthfully reported innocent with a ratio of:  0.6111111111111112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QTXyuufWYaPX",
        "outputId": "dc137488-ab67-42ef-9ce2-e72a3f204dc4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.plot(range(len(zero_zero_probs)), zero_zero_probs)\n",
        "ax1.set_title(\"Probabilities over rounds for sending signal 0 given state 0\")\n",
        "\n",
        "fig2, ax2 = plt.subplots()\n",
        "ax2.plot(range(len(zero_one_probs)), zero_one_probs)\n",
        "ax2.set_title(\"Probabilities over rounds for sending signal 1 given state 0\")\n",
        "\n",
        "fig3, ax3 = plt.subplots()\n",
        "ax3.plot(range(len(one_zero_probs)), one_zero_probs)\n",
        "ax3.set_title(\"Probabilities over rounds for sending signal 0 given state 1\")\n",
        "\n",
        "fig4, ax4 = plt.subplots()\n",
        "ax4.plot(range(len(one_one_probs)), one_one_probs)\n",
        "ax4.set_title(\"Probabilities over rounds for sending signal 1 given state 1\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Probabilities over rounds for sending signal 1 given state 1')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAG0CAYAAAB+GyB3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8tklEQVR4nO2dd3wUdf7/X7PZVEJIIkGQEjqcdyhipUlTQcGCgAU7ivVOvaLn4Sl4Byp6KtbT36l4fBXLqYCgoIjY0BNB8AQRQUBFEiCQQgik7H5+f2x2MrM727Iz85nNvJ6Ph5KdnZ2Zz2dn3vv+vKsihBAghBBCCCFS8Mi+AEIIIYQQN0NljBBCCCFEIlTGCCGEEEIkQmWMEEIIIUQiVMYIIYQQQiRCZYwQQgghRCJUxgghhBBCJEJljBBCCCFEIlTGCCGEEEIkklLK2AsvvABFUfDCCy9Ydo4PP/wQiqJgxowZcX9mxowZUBQFH374oW67oigYPnx4XPtGY8eOHVAUBVdeeWXcnyHuxOpn5LHHHsPRRx+N7OxsKIqCOXPmWHKelkAkWTJ8+HAoiiLnokzASK45lSuvvBKKomDHjh2yLyUuunbtiq5du8q+DCKBhJUxRVF0/6WlpaFt27YYOXIk5s+fb8U1uoJUEnDEnbzyyiu45ZZbkJWVhVtvvRXTp0/HKaecIvuyCDGNTZs2Yfr06Tj33HPRpUsX9XeuoaFB9qW1OMxWlO0w1gSx4j7xNveD06dPBwDU19fju+++w6JFi7By5UqsWbMGDz/8cLMvKBX57W9/i4suughdunQxdd8gHTt2xKZNm9CmTZtkLpOQpFiyZIn671FHHSX5alKXefPmoaamRvZlNJtNmzYhJydH9mVYwrvvvou//e1vSEtLQ69evZCVlYXDhw/bdv4VK1bYdi7SfKy4T5qtjIWa3lesWIHTTz8dc+bMwc033+wqU2vbtm3Rtm1b0/cNkp6ejr59+zbn0ggxjV27dgEAFbEkSWQh5kRasiw688wzMXDgQBxzzDHIzs5G165d8eOPP9p2/h49eth2LtJ8rLhPTIsZGzVqFPr27QshBL788ksA+vio+fPn4+STT0Zubq5OUSspKcFNN92Erl27IiMjA0VFRTj//POxdu3aqOd7++23MWjQILRq1QoFBQWYOHEitmzZErbf999/jzvuuAMnnHACioqKkJmZieLiYlx77bXYuXNn1HN8/vnnOO2009CmTRu0bt0ao0ePxpo1a8L2SyQOLHTfoGkVAD766COdCzio8EaLGaupqcF9992H/v37o1WrVsjNzcXAgQPx8ssvh+0rhMC///1vDBo0CEVFRcjKykLnzp0xevRovPrqqzGvPUhlZSX+8pe/oE+fPsjKykJBQQFGjx6N999/X7ffK6+8AkVR8Pvf/97wOLW1tSgoKECHDh3CzLsvv/wyRowYgfz8fGRlZeFXv/oVZs6cidra2rDjBF28paWluOaaa9CxY0ekpaXFNFdrY3pWr16NsWPHorCwUGc6r62txf33349+/fohJycHeXl5GDp0KF577bWoxzPCKB5Ea1pfuXIlhg8fjtatWyMvLw9jx47Fpk2bDI+1detWTJo0CQUFBWjVqhUGDRqEt99+O+JY//e//+Hiiy9G165dkZmZiaKiIgwYMAC33nor6uvro85T8J5duXIlAH2ogpYVK1ZgzJgxKCwsRGZmJnr37o077rgDlZWVYccMxk3V1dXhb3/7G/r06YPMzMy44iI/+eQTnH322ejUqRMyMzPRvn17nHLKKbjnnnvC9k3k+dB+f+vXr8fYsWORn5+PnJwcDBs2DJ999pnh9ezevRtXX301jjzySGRnZ6N///7497//HfH6jWLGmnvukpISXHXVVWjXrp3u3InGvtbV1eGxxx7DgAEDUFBQgJycHHTt2hXnnntu2HMdKaQi0WsJzkNDQwPuvfde9OrVC5mZmejcuTP+/Oc/o66uLuwcCxcuxKWXXorevXujVatWaNWqFY4//ng89thj8Pv9cY01Gn369MHJJ5+M7OzspI8FBH5/JkyYEPacRnKphcqI+++/H4qi4NFHHzU8/q5du+D1enHCCSfotjc0NOCpp57CKaecgry8POTk5OC4447DE088ETZP2t+XHTt24KKLLkLbtm2RlZWFE044QbWIx0s8z6eiKOoz0q1bN1WeaMe+du1a3HLLLTj22GNRWFiIrKws9OrVC3/84x9RXl6uO+fw4cNx1VVXAQCuuuoqnYzSukETmZdomH2fAElYxowQQgBAmKB56KGHsHz5cpx99tkYMWKEKpy3b9+OIUOGYNeuXRg5ciQuvvhi/Pzzz/jPf/6Dt99+G2+88QbGjRsXdp4333wTS5cuxfjx4zF8+HCsX78eb7zxBlauXInPPvsMffr00e379NNPY8SIERg0aBAyMjKwceNGPPvss1i8eDHWrFmDjh07hp3jiy++wH333YfTTjsNN910E7Zu3Yo333wTH3/8Md577z0MHTrUlDnr378/pk+fjnvuuQfFxcW6H6NYMWQVFRUYOXIk1q1bhwEDBmDKlCnw+/149913MXnyZGzcuBEzZ85U97/zzjtx3333oVu3brjgggvQpk0blJSU4Msvv8R//vMfXHjhhTGvt6KiAoMHD8a3336LE088EbfeeivKysrw2muv4YwzzsA///lPXHfddQCA8847D23atMH8+fPx4IMPwuvV326LFi1CRUUF/vjHP+remzJlCubOnYtOnTphwoQJyM/Px3//+1/cddddWLFiBZYvXx52rP379+OUU05Bbm4uzj//fHg8Hhx55JExxwMElO777rsPQ4YMwZQpU1BWVoaMjAzU1dVh9OjR+Oijj9C3b1/cdNNNqKmpweuvv44LL7wQ69evx7333hvXOWKxZMkSLFq0CGeeeSauv/56fPvtt3jnnXfw5Zdf4ttvv9VZU7ds2YKBAwdi3759OPPMM9G/f39s3boV5513Hs4888ywY//vf//DySefDEVRcM4556Bbt26oqqrC1q1b8dRTT2HmzJlIT0+PeG3B+/CFF17Ajz/+qIYoaHnmmWdwww03oFWrVpg0aRLatWuHDz/8ELNnz8bixYuxatUq5Ofnh31uwoQJ+PLLL3HmmWfivPPOQ7t27aLO07JlyzB27Fjk5eXhnHPOQceOHbF//35s2rQJTz31lO7aEn0+gqxZswYPPPAABg4ciGuuuQY//fQT3njjDYwaNQrr16/XyZeysjIMGjQI27Ztw5AhQzBkyBCUlJTg+uuvxxlnnBF1LEYkcu49e/Zg4MCB+PHHH3Hqqadi0KBBKC0txY033pjwua+88kq8/PLL+M1vfoPLL78c2dnZ2LVrFz799FMsW7YMp512WtTPJ3MtkydPxieffIIzzzwTeXl5eOedd/DAAw9gz549mDt3rm7fO+64Ax6PByeffDI6duyIyspKfPDBB7jlllvw5Zdf4v/+7/8SGreVfPfddxg0aBDKy8sxduxYHHPMMdi2bRvGjx+Ps846K65jXHbZZbjzzjsxb9483HLLLWHvv/jii/D5fLrfjfr6epx99tl499130adPH0yePBlZWVlYuXIlfve73+GLL74wnKcff/wRJ510Erp3747LLrsM+/fvx6uvvqoq5CNGjIh5vfE+n9OnT8fChQvx9ddf45ZbblFlg1ZG/Otf/8KCBQswbNgwnHbaafD7/Vi7di0efvhhLF26FF988QVat24NIHD/5ufnY9GiRTj33HPRv39/9TjBYzZ3XmxDJAgAYfSx5cuXC0VRhKIoYseOHUIIIaZPny4AiJycHPHVV1+FfeaMM84QAMTMmTN121etWiXS0tJEYWGhOHDggLp97ty56vkXL16s+8ycOXMEADFy5Ejd9p07d4rDhw+Hnfvdd98VHo9HXH/99brtK1euVM/x+OOP695buHChACB69uwpfD6fuj04zpUrV+r2ByCGDRum25bIvkG2b98uAIgrrrhCt/2KK64QAMTs2bN12w8dOiRGjx4tFEUR69atU7cXFhaKjh07ioMHD4adY+/evYbnDuXaa68VAMS1114r/H6/uv37778XeXl5IiMjQ2zfvj1s/9DvSwghzjrrLAFA/O9//1O3Bb/j8ePHi5qaGt3+wbmbM2eObnvw+7rssstEfX19XOMQQv9dP/3002Hv33vvvQKAOPPMM3XH3b17tyguLhYAxKpVq8KON336dMPzFRcXi+LiYt224HjT0tLE+++/r3vvjjvuMPx+Tz/9dMN5CN6fAMTcuXPV7X/4wx8EALFw4cKwa9q/f7/uXo7GsGHDDJ/9HTt2iIyMDNG6dWuxadMm3Xs33HCDACCmTp1qeKx+/frFfe8JIcT5558vAIj169eHvRd6nESfD+39oJ0/IYR4+umnBQBxww036LZPnTpVABC33nqrbvuXX34pvF6v4f1gNI/NOfeUKVMEAHH77bfrtq9fv15kZGREvRe1VFRUCEVRxPHHHy8aGhrC3i8rK9O9NpJVzbmW4DwMGDBA7Nu3T91eXV0tevToITwejygpKdF9ZuvWrWHX5/P5xOWXXy4AiP/+97+694L3gFYmJULwOU9ErgQZOXKkACCeeuop3fZ33nkn4ndtJCOCv5PffPNN2DmOPvpokZGRofuOgnLyt7/9re77bGhoUL8nrSwI/r4AEDNmzNAdf9myZaoMjIfmPJ+RvpsdO3YY3o/PPvusACDuv/9+3fagLA2d0yCJzksiJHOfBGm2MjZ9+nQxffp0MW3aNDFhwgSRlpYmAIjf//736r7BwYcKKiGE+PnnnwUA0aVLF1FXVxf2/qWXXioAiH//+9/qtuBkhypcQgQmtEePHgKAqgzGol+/fqJbt266bUGhGKpwBQkKkA8//DBsnHYqY2VlZSItLU2ccMIJhp9Zv369ACBuu+02dVthYaHo2rWroXIaD7W1tSInJ0fk5ubqhGeQv/71rwKAuOeee9Rtq1atEgDExIkTdfuWlJSItLQ0cdxxx+m29+/fX3i9XlFeXh52/IaGBnHEEUeIE088UbcdgMjIyBC7d+9OaDzB77p///6G7/fs2VMoihKmYAjRJBCuuuqqsOM1Rxm75JJLwvbftm2bACAmTJigbgs+N926dTMUVMH700gZe/fddw2vK14iKWMzZ84UAMRf/vKXsPf2798vWrduLbKysnT3XfBYiQq/oLDfvHlz1P2a83wEv7/BgweH7V9XVye8Xq84/vjjddtycnJE69atRUVFRdhngj82iShj8Z67trZWZGdnizZt2oiqqqqwz1xzzTVxK2OVlZUCgBg0aJBugRWJUFnV3GsJzsPy5cvDPnP33XdHXMQZsXbt2jDZI4Q8Zeynn36K+jty2mmnxa2MvfTSSwKA+NOf/qTb/uWXX6oL1yA+n08UFhaK9u3bG15zeXm5UBRFTJo0Sd0W/H0pLi42lCldunQRRxxxRDzDjvv5FKL5343f7xd5eXlixIgRuu3RlLHmzEsimKGMNdtNGfT/KoqC/Px8DB06FFdffTUuvfTSsH1POumksG3r1q0DAAwdOtTQRTJy5Ei8+OKLWLduHS6//HLde8OGDQvbPy0tDUOGDMEPP/yAdevWobi4GAAghMBLL72EF154AV9//TXKy8vh8/nUz2VkZBiOb+jQofB4wkPqhg8fjo8++gjr1q0zvA67+PLLL+Hz+SLGhQTjgLQxR5dccgkef/xxHH300bjgggswbNgwDBw4MO4szc2bN6OmpgaDBw9GYWFh2PsjR47EzJkz1e8WAAYNGoTevXtj8eLFKC8vR0FBAQDgpZdeCjOv19TU4Ouvv0bbtm0j1q/KzMw0jKPq2rVrTBdXJIzuzwMHDmDr1q3o2LGjYcDyyJEjAUA31mQIjfkAgM6dOwOALj4ieL4hQ4YgLS0t7DPB+1PLhRdeiEcffRTnnXceJk6ciNNOOw2DBw82LVj4q6++AtA0J1oKCgpw3HHH4eOPP8Z3332HY489Vve+0dxH45JLLsGbb76Jk08+GRdeeCFGjBiBwYMHo1OnTrr9mvN8BDH6LtLT03HkkUfqvovvvvsONTU1GDp0qOEzNHz48KixY0bEe+7Nmzfj0KFDOOGEE1RXjZYhQ4bg2WefjeuceXl5OPvss7F48WL0798fEyZMwNChQ3HyySfHlTWZ7LXEe+8DwL59+/Dggw/inXfewbZt23Dw4EHd+7/88kvM67WD9evXAwAGDhxo+DsyZMiQsFi8SIwfPx5t2rTBSy+9hPvvv1997oP3llaGfv/999i/fz969epl6IIHgOzsbMP7vn///oYypXPnzvj888/jutZ4n894qK+vxzPPPINXXnkF3377LSorK3VxXYl818nMi100WxkTjfFh8dC+ffuwbcG4sQ4dOhh+Jri9oqIi7L1IsUDB82gDhv/whz9gzpw56NChA0aPHo2OHTuqQXfBGBgjEjmHDPbt2wcg8KMTTJgworq6Wv37kUceQffu3TF37lzcf//9uP/+++H1enHWWWfhoYceQs+ePaOes7nf2RVXXIE777wTr7zyCm644QYAAUGSnp6OyZMnq/uVl5dDCIG9e/caBmNHw+geS+azydyfzcEonioYF6ddPASvK9b9qeWkk07CJ598glmzZuH1119X4yL69OmD6dOn4+KLL07q2pOZq0S/t/PPPx9LlizBQw89hOeffx7PPPMMAOD444/Hfffdh9NPPx1A856PIEbfBRD4PpL9LmJh1rnjjZcM8uqrr2L27NmYP3++GteTlZWFiRMn4h//+EfU4yV7LfHe+xUVFTjxxBOxfft2nHTSSbj88stRWFgIr9eLiooKPProo4YJPjIw8/vJzs7GBRdcgH/961947733cOaZZ6Kurg4vv/wyioqKdHGiwft+y5YtUWVoovd9vMHt8T6f8XDhhRdiwYIF6N69O84991y0b98emZmZAIA5c+Yk9F0nMy92YUsFfqNq08GVZGlpqeFnSkpKdPtp2b17t+FngscKfmbPnj147LHH8Jvf/AabN2/Giy++iNmzZ2PGjBmYMWOG+sUaEe85ZBE8/+9//3uIgLvZ8L9gBhwQsB7eeuut+Prrr7F792688cYbGD9+PN566y2MGTMm5s3d3O/ssssug8fjUVdy69atwzfffIOzzjpLF5ge/Nxxxx0XdUxGC4FkKpqbdX8GV8CRCv+ZobgFzxfr/gxl4MCBWLJkCcrLy7Fq1Srcdddd2L17NyZPnhz3Cj3WNTXnWW7O9zZ27Fh88MEHKC8vx4oVK/D73/8eGzduxLhx4/Dtt9/qzpXI85Eozf0uzCAvLy/quSNtj0R2djZmzJiB77//Hj/99BNefPFFDBkyBC+++CImTpxo67VE4tlnn8X27dsxffp0fPHFF2ryyYwZM+JKPrITs+fkiiuuANBkDXv77bexb98+TJ48WedZCt6T48ePj3rfb9++PeExxUs8z2cs1qxZgwULFuC0007D5s2bMXfuXNx3332YMWMG7r77bsNM22g4YV5iIa0d0nHHHQcA+PTTTw1/vIJCcsCAAWHvhbphgMAK6tNPP9Ude9u2bfD7/TjjjDPCzOc7d+7Etm3bIl7fp59+argaCJakCJ7DLDwej24VGIuTTjoJHo8Hn3zySbPO165dO5x//vl47bXXMHLkSPzwww/YsGFD1M/06dMHOTk5+Prrrw0Vi0jfWefOnTFy5Eh88cUX2Lx5sypQggImSG5uLn79619j48aN2L9/f7PGZRatW7dGjx498MsvvxiWTDEaa9AF+/PPP4ftv3XrVlOsqdrnxuh+iVVeJTMzE4MGDcLf/vY3PPbYYwACWa1mXJPRuSsqKrB+/Xq1PImZtGrVCiNHjsTDDz+MadOmoa6uDkuXLgWQ/PMRD3379kVOTg7Wr19v+N0m0vKsOefOzs7G//73Pxw4cCDs/aAsbA6dO3fGJZdcgnfffRc9e/bEp59+qloW7L4WLVu3bgUQyMANxeg3QSbBbL7PP//c8Hck0TkZPHgwevXqhUWLFqGysjKiDO3bt6+afR6rZI3VRHs+AaguUSM5FvyuzznnnLDM+dWrV+PQoUNhn4l2PCfNSySkKWOdOnXC6aefjh07doTFB33xxReYP38+CgoKMH78+LDPfvDBB2G1T5544gn88MMPGDFihBovFqxZEvrDVV1djalTp0ZtXbBlyxY89dRTum2LFi3CRx99hJ49e5pW2iLIEUccYfgjHol27drhkksuwZo1a/D3v//d8Ab84YcfVE2/trYWq1atCtunvr5eVXxixYdkZGTgkksuwYEDB3DXXXeFneuxxx5Deno6LrvssrDPBuMannvuObz88sto27atYdmSP/zhD6irq8OUKVMMFb7y8nI1RslqpkyZAiEEbrvtNt38lpWV4e9//7u6T5C+ffsiLy8PixYtwp49e9Tthw4dws0332zKNQWfm+3bt+OJJ57QvRe8P0P57LPPDIVXcHWebDX1Sy+9FOnp6Xj88cdVIRrkrrvuQlVVFS699NKoluh4+fjjjw2f29CxJPp8NIf09HT1eQiNS1uzZg1eeumlZh87FhkZGbjwwgtRWVkZFgPz9ddfY968eXEfa+/evfjmm2/Cth88eBDV1dXwer0RY2vNvpZoBOV5qJK7bt063Hfffaacwyy6dOmC4cOHY+vWraqrLsiyZcuaZY2+4oorcPjwYTz11FN45513cMwxx4QZBbxeL373u9+hpKQEN998s+FzX1JSEreFKlHifT6BwG8eAPz0009h+0f6rvfs2YObbrrJ8NzRjid7XuLB1DpjifL0009j8ODBuO222/Dee+/hhBNOUOuMeTwezJ071zAg9Oyzz8b48eMxfvx49OzZE+vXr8fSpUtRWFioU6Dat2+Piy66CK+88gr69++PM844A5WVlVi+fDmysrLQv39/NdAylDFjxuCPf/wjli5dimOPPVatM5aVlYXnn3/eMCgzGUaNGoVXXnkFZ599NgYMGID09HSceuqpOPXUUyN+5oknnsCWLVtw99134//+7/8wZMgQHHnkkdi1axc2bdqEL7/8Ei+//DK6deuGQ4cOYciQIejZsyeOP/54FBcX4/Dhw1i+fDk2bdqEc845Jy7Lxf33349PPvkETzzxBL788kuMGDFCrTN24MABPPHEE+jWrVvY58aPH4+8vDzMmTMH9fX1+N3vfmeYuDFlyhSsXbsWTz31FHr06IHRo0ejS5cu2L9/P7Zv346PP/4YV111FZ5++unEJrgZ/OlPf8LSpUuxaNEiHHvssTjrrLNQU1OD//znP9izZw9uv/12DBkyRN0/PT0dt9xyC/7+97/juOOOw/jx49HQ0IDly5fjqKOOMq1y/ZNPPomBAwfi1ltvxXvvvafenwsWLFADsbU88MAD+OCDDzB06FB069YNubm52LhxI5YuXYqCggJce+21SV1P165dMWfOHNx0000YMGAALrjgAhQVFeGjjz7C559/jr59+2L27NlJnSPIzTffjF9++QWDBw9WC0WvXbsWH3zwAYqLi3HRRRep+ybyfDSXe++9FytWrMCcOXOwZs0atc7Yq6++irPOOgtvvfWWGcM25P7778cHH3yABx54AF988QUGDRqEkpISvPbaazjrrLOwcOHCuOTUL7/8guOOOw79+vXDMcccg86dO6OqqgpLlixBaWkpbr75ZkM5bMW1ROPyyy/Hgw8+iFtvvRUrV65Er169sGXLFixZsgTnn39+QoWrI1FWVoY//elPutcAcPXVV6su9TvuuCOuLgRPPvkkBg8ejBtvvFFVnrZt24Y33ngD5557LhYtWpTQnFx22WW4++67MX36dNTX14dZxYLcdddd+Prrr/H0009j8eLFGDlyJDp27Ig9e/Zgy5YtWLVqFWbNmoWjjz467nPHSyLP56hRo/Dggw9i6tSpmDBhAlq3bo38/Hz89re/xYknnojBgwfjzTffxKBBgzBkyBDs3r0bS5cuRZ8+fQxl6cCBA5GTk4M5c+Zg3759aszm7373O7Rp08bUeTHzPlFJNP0SMK4zZkSkMg5adu7cKa6//nrRpUsXkZ6eLo444ghx7rnnitWrV4ftq01dXbx4sTjllFNETk6OaNOmjTj//PMN02kPHjwopk2bJnr06CEyMzNFp06dxI033ijKysqipphPnz5dfPbZZ2LUqFGidevWIjc3V5x++umG12VGaYvdu3eLiy++WLRr1054PB5dKnikOmNCBNLKH3/8cTFw4EC1zlfnzp3FyJEjxSOPPKLWn6mrqxOzZ88WY8aMEZ07dxaZmZmibdu24uSTTxb//Oc/RW1tbdixI1FeXi5uv/120bNnT5GRkSHatGkjTjvttJilE66++mr1/lmzZk3UfRcvXizGjh0rioqKRHp6ujjyyCPFiSeeKO68886wUhNG8xwPsUpRCBGoSTVr1izx61//WmRlZYnc3FwxePBgMX/+fMP9/X6/uO+++0T37t1Fenq66Ny5s7jtttvEwYMHo5a2iFQbJ9LYtmzZIiZMmCDatGkjcnJyxCmnnCKWLFlieLx3331XXHnlleJXv/qVyMvLEzk5OaJ3797id7/7XdxlYISIXNpCe57TTz9d5Ofni4yMDNGjRw9x2223GZYpiXWsSLz66qvioosuEj179hStWrUSrVu3Fr/+9a/FtGnTxJ49e8L2j/f5EKJ5pUmECJRpueqqq0Tbtm1FVlaWOPbYY8XcuXMjHi+W3Enk3Dt37hSXX3657twvvPCC+M9//iMAiEceecTweFrKy8vFPffcI0aMGCGOOuookZGRIdq3by+GDRsm5s+fH1buItI9mei1RLsHIj0XGzduFGeffbYoKioSOTk5YsCAAeJf//pXzFqM8ZZP0NbcivRftN+zUDZt2iTGjx8f9pw++OCDAoBYsGCBbv9I33OQUaNGCQDC6/WK0tLSiPv5/X4xb948MXLkSFFQUCDS09PFUUcdJQYPHixmzZolfvrpp7AxG/2+CJHYs5ro8/nQQw+Jvn37qrXotGPft2+fuOGGG0RxcbHIzMwU3bt3F3/5y18iylIhhFi6dKk45ZRTRKtWrdTvS/vdJzIv0TD7PhFCCEWIBNIiCSGEOJ4777wT9957L5YtW4bRo0fzWhzGJZdcgvnz5+O7777TdVUg7oXKGCGEpCi7du0Kc9l88803auu3X375BVlZWa67Fifg9/uxZ8+esBInK1aswOjRo9GnTx9s3LhR0tURpyE1ZowQQkjzOeGEE9CzZ0/85je/QatWrbBlyxa8/fbb8Pv9eOaZZ2xVfpx0LU6grq4OnTt3xogRI9C3b194vV5s3LgRy5cvR0ZGBp588knZl0gcBC1jhBCSotxzzz1YuHAhduzYgQMHDiA/Px+nnHIK/vSnP6kN3t14LU7A5/Ph1ltvxQcffICdO3eipqYGbdu2xamnnoo77rjD9PJIJLWhMkYIIYQQIhFpdcYIIYQQQgiVMUIIIYQQqVAZI4QQQgiRCJUxQgghhBCJuKq0RXl5edR+lM2hqKgIe/fuNfWYToLjS204vtSG40ttOL7k8Xq9KCgosPQcTsBVylhDQ4OpHduDPagaGhrQEpNSOb7UhuNLbTi+1IbjI4lANyUhhBBCiESojBFCCCGESITKGCGEEEKIRKiMEUIIIYRIhMoYIYQQQohEqIwRQgghhEiEyhghhBBCiESojBFCCCGESITKGCGEEEKIRKiMEUIIIYRIhMoYIYQQQohEqIwRQgghhEjEVY3CnYzw+SA+eQ+o2AcIABCA2nxVNG5D4zbNa+1+oZ9rUwDltHOgpGeEn6+hAWLF4sD5ACAzC8qIsVDaFJg/uBRAlOyE+OkHoK4WqK8L/FdXB9TXBuY1OO9A0zyjcbvRd5GZBWXEWVDyj7BzGEkhDlYD+/YA+/dCHKgEGhoC89BQD9TXB/4V/sadwz6t+VMYbo64T3NRFJS3agXfwYPJHS8nFzjyKChHHgX4fEBleWD8wbE3NAC+hsC/fl/45yOd23BzpH0Ntps1PqciY3xF7aEMOxOKlz99zUUcqAL274EoL0P1egH/nj0QtYeBuloo514CJS1N9iWmJLwjncJ3/4N46Z+mH1Y5siMwYKDx+V6fq98mAGX8paZfg9MRhw/BP/PWgPJlJr4GKBOvMveYJiN2bodY+znEus+BX36UfTkJU23isZyo7pg5PiciY3ziu2/gue42KN50CWdPTcTOHRBrP4P46jNg10/q9vKQ/ZQxE4CcVvZeXAuByphDEDWNYin/CChB5UlR9P9CARSD19Ds1/harPkU2FsKcfhQ8F09h2sC/xa2BQraAj9817TNbdRUBxQxRQH6nRCwJGZkAOmZQHo64Gn05isKmuY6+GFF8z0F/ie2fgts+RY4fMjWYSSC8Psh/vM8xPtv6d9o3QYoLALy8oH0DCjp6YA3HUjPCPzr0UQ2hN1Ymg2KEmG/CPs0k9zcXFRXJ/GTLgRwoBKi9Bdgb0lgjHn5QF4+lMwsIM0LeBv/S0sH0jwwGHjksRhujrRv+Pakx+dwbB1fQz3EB28D6/8L/9Oz4bnuz4H7m0RECAGx8EWId/6jf6NNAVBYhKx27VELBcjIDPznSf6ZditUxpyCv9H9074jPBdfm/ThfDt3AHtLI5r/RXB7UQcovX4N8cN3TS4otxGc+/R0pP3uruQPt+RViC3fOta1JOrrIZ5/JKCwA8CxJ0EZMAjKsSdBaZUr9+ISQFEU5HfogEMlJU33cwuC4zMf8av+8D91L/D1aojnH4Fy3e22nDcVET4fxItPQXy6PLDh2JOgHD8YyjEnQmmVC0VRUNShA0pa6P1pN1TGJCAOVgOHD0E5okizsfFm9piUUxFcZUdSsIIKiMej2delD1Rw3IrZc++8+RT1dfA/eg+w+RsgzQvlqlvgOXmY7MsixBaU3wyA58Zp8D86A2LNpxBX3QIlI1P2ZTkOIQT8z8wG1v0XUDxQLrsRnqFnyL6sFg2VMZsRleXwz/w9cLAangdfaLJEqAqBSWbeWAqB9nxB07LfecqDLWgVUzMIHsfvPEujWPV+QBHLyobnxmlQfnWs7EsixF56/brpbwc+o47g520BRSzNC891t0M57hTZV9TiYWkLGxF+H/z/+gdQsT+QqVVVoX0z8K9p1hmP/rhhF6NRxmLt29KxWxGWiPjkPQCAcu5kKmLEnWjjmhz4jDqCw4cD/xYdSUXMJqiM2Yh46+WAVULdoFF+zFYIYlm7tMqfg5UHW/DbrAhLQvy4FfhpG+D1QjllhOzLIUQO2uecljFjzA7dIDHhTNuE2LiuKSMlqPxoBYHZrrJ4Y8a0ljG3CiahmQszMPp+HYD4uNEqNmAQlNw8yVdDiCR0ljFnPaOOwWyZSGJCZcwm/G/+GxACyrAxgdR5IKQ4prk3v6IqWDFixjyeJuHkVsFkdvKEx3mWRlF7GGL1RwAAhYG4xM1orT0OekYdhdmeGhITKmM2IEp3BtxDaWlQzrvU2GrltypuKVLMmNYy5jzlwVYsc1M6Zz7Fmk8Ddc/adQD69JN9OYRIQ1FoGYuJ2TKRxIQzbQNi9ceBP44+LuAeMvqxNttH74mhEPi1lrEYVrSWjtkmeQdmU6qB+0NO1/8YEeJGYnkO3I4wOWyGxIQzbTFCCIjVnwAAlJOGBjYqBsH1jTe/YlrMGLMp40Z1U5prlXRKIURRfSDQYQGAMmiU5KshxAE4MJTAUdBNaTtUxqzmp23A7l8CrWX6nxzY5jFQfmyvM9ao/DGbUuMibqHZlPWNPTfT0lzbCJ4QHQ5NsnEMfpPjaElMONMWo7oojzkBSlZO4G8jQWB2nbGYpS3CLWPCKcqD3ZhtkneacsvMKEL0GC2ISROUGbZDZcxChN8P8WXARek56dSmN4xixvxmu8qCMRE+4/e1AZpuXyX6TRY8TptPs7NFCUl1HJhk4yjoprQdSmcr+eE7oLwMyM4B+p3QtN2olIRVta5iuCnhURg/YXppC4cJemZGEaInVra526HMsB3OtIWItasAAEr/k6GkZzS9Ec0yZld5BcMAfocoD3Zjeisqhwl6sy1/hKQ6zKaMjnaxTmyByphFCCEgvl4NAFAGDNS/GTVmzOTyChEr8GusQU5zq9mMMFtZcVqpELY2IUSP0+I6HYagzLAdzrRVlO4EynYDXi/QN6Qhs5EgMNtVFq+bUvE4z61mN6ZnsjosOJjBuITocXvXkViY3Z6PxIQzbRHif18G/ujTD0pWtv5NI+XH9GzKONshsc6YC7IpGcBPiA639+ONBQP4bYfS2SLE/9YAAJRjTgx/00gQmJ5NGW87JNYZM92N57QYPApWQvS43RsQC8oM26EyZgHiYDWw9VsAgKLNogxipCjZXfRVq/y5XRkzOWZMUWu8RSgrYjd0ORCix2lJNk7DbE8NiQln2gLExq8CP4BHdYFS1D58BzvclLHM8No4Igf2UrQVs6tNO025ZcwYIXqcZr12GszAth0qY1bQGC9maBUDjFdlVhUejZVNqXiaGke7dZVoesyYwwS92WVTCEl1jPoDkyYYZ2o7nGmTEX4fxIavAESIFwMiNAo3+QczViFXrbXE7TV3zHYRO83SaLaySUiq4/YFaCzoprQdzrTZbNsMHDwA5OQCPfoa7xPNTWmadSYt/BxatCsft/dps8wq6RDllsG4hOhx2oLJaZidUEZiQmXMZMQPmwN/9O0HJS3NeKdgY26/hQH8ahA5syljYlW8nlPmk/EfhOhx2jPqNLiAsx0qY2bz0zYAgNKlR+R9DGPGrCptwXZIMbGs4K5DVt10ORCih0Vfo9M4Lwplhm1wpk1G/BxUxrpH3snObMqY7ZCU2Fa0lo7pragcFhws6HIgRIfbF6CxoDXddqiMmYioPRxogwQAiVrGrKozlpCb0qXKmN9mRdhmBLMpCdHj8n68MWE2pe1wps1k547ATdymAEqbgsj7GWVTmq4QxCr6ymxKFatcxE4R9MymJESP2+NkY8HQBtvhTJuIaIwXQ+coLkoA8BhkOpruKothndGW0nC7Zcz0siIOc4HQ5UCIHropo8MAftuhMmYmP8cRvA9oLCeadjlmx/XEbBSusZa4fZXY0ou+UrASosftcbKxYGkL26EyZiLixx8AxAjeB4yVH9ObVSeQTWlkqXMTdnc/sBu6HAjR47C4TsdBmWE7nGmTEA0NwK4fAy9iKmMGlhOzmznHEja60hYOUx7sxu4m7XbDYFxC9DgtlMBpMLTBdiidzaLkZ6ChAchuBbQ9Mvq+RjVu7FYItMqf00ox2I3ZbkqnVff2001JiA63L0BjwQWc7XCmTUL8FHBRokv3psbbkTB0U9pcBd6w6KtLBZPf5AKHThP0ojE2kYKVkABsFB4duilthzNtFsHK+7EyKQEowRgtw0bh5hYeFdokAS3aUhpOK8VgN6ZbJR1WKoSWMUL0OC3Jxmkw6cd2qIyZhFrWoji2MtZkOdEoSqrb0K52SAZ1xtwqmEyP13NozBgFKyEB2A4pOn66Ke2GM20Cwu8Hft4OAFA6xyhrARibyK2qdRWxtIXmYfM4THmwG8vqjDlE0KsxcREa1xPiNhqfdeFWb0AszK57SWJCZcwMysuA2kNAmhdo3zH2/o7KpvQwZsz00hYOszQyM4oQPcymjA5lhu1QGTODutrAv5lZUNLisD4YBHgLadmUCi1jwmQXsdNcIHRTEqLHaUk2TsNsbwGJCWfaDBoaAv96vfHtb6T82JxNKYyyKd1qslddtia58ZyWqcXMKEL0OO0ZdRrsZ2s7nGkz8DUG4qfFq4wFsyntqDMWyU1pkE3pWsuYRdmUTll1s7UJIXqcFkrgNGhNtx0qY2bgS9AyZqQoWVTaIqK1y2+QTelWy5hV7ZCcsuqmy4EQPXRTRocyw3Y402bQUB/4N17LmNGPtWUB/PEUfXW3ZUyY7iJ2mKD3s+grITqc1iXDaZhdaonEhNLZBITqpowz5shIUTI7lThWIVfD0hYuFUxmt/5wWuN1uhwI0UM3ZXQoM2yHypgZqAH86fHtb2Q58ZtrFlZipW7rYsZcLpisclM6RbmlYCVEj9sXoLHwM+nHbjjTZhCMGYvXMmakKJmdvRIriFwb1O20IqV2Y7bgcVqpEGZTEqLH5aEZMWE2pe1wps0g4QB+A+XH7jpjRtmUTgk4txthcrah03pTNo5PoWAlBACgOO0ZdRq0ptsOpbMZBN2UcZe2MIjnMr0lTyxlzKDOmFtXiS0+gJ/VtAnR4bRn1GnQmm47nGkz8CWojBlZrRpvfsV060w87ZBiBPu3dKyyjMEhve8Y/0GIHrcvQGPB2oS2Q+lsBolW4DcykfstclNGrDPma7wUhcGsVsWMAc4Q9mYrm4SkOrHqMLoduiltJ07twVyWLVuGxYsXo6KiAsXFxZgyZQp69uwZcf+3334b7733HsrKypCXl4eTTz4ZkydPRkZGho1XHYVEA/gNi77a2w7J0E2JQJskxW0PoFUV+IHG79WkNkvNhQUcCdHjtC4ZToPWdNuxfaY/++wzzJs3DxMnTsTs2bNRXFyMWbNmobKy0nD/Tz/9FPPnz8ekSZPwyCOP4Prrr8fnn3+Ol19+2eYrj0KjZUxJi7e0hYGiZHZcT6wMSW1tLe053SiczM4ccpxljAUcCdHhtIxnp8FsStux3TK2ZMkSjBo1CiNGjAAATJ06FV999RVWrlyJ8847L2z/zZs3o0+fPhgyZAgAoF27dhg8eDC2bNkS8Rz19fWor69XXyuKguzsbPVvs1CPFSz66vXGd/y0JmVMCY0fS0sz5RqFx+Acuh2C1pI0KBqLniKaxhX6b0tDHZ8QEAhkG5oyVo92PuVZGsPuLcWk8TkE19yfHJ8FJw/IRyufz5T+/jQZ2JGuP6XH50BsVcYaGhqwbds2ndLl8XjQr18/fP/994af6dOnDz755BNs3boVPXv2xO7du7Fu3ToMHTo04nkWLFiA119/XX3drVs3zJ49G0VFRaaNRUtuThaqAOS0zkNhhw4x96/Mywvsn52l7l/q9aIeQOERRyA7jmPE4vCendgLID0tDe0Njrfbm4a6xvNlduiAXxq3d2h/JJR0vfu3ffv2SV+Pk8nOzMJBALl5eWhjwtz7aw+r89n+yCPhyc5J+pjJkNsqJ3C/tcqN6/5MNVr6/cnxmc/+VrmBZz63lSnPfDRS8fsry8zEIQB5bfLROsb8pOL4nIitylhVVRX8fj/y8/N12/Pz87Fr1y7DzwwZMgRVVVW46667AAA+nw+nn346zj///IjnGT9+PMaNG6e+Dmrue/fuRUMw2N4EFEVB+/btUV1RAQCoqatDbUlJzM/5qw8G9q+uVvdvqKsFAOwvr4AnjmPEPMf+cgBAfX0dSgyO11BbFzhfRTmU3XvU7SW7dkHJyATQNL7S0lKIFmjOD46vpibwfVRXH0SNCXMvNFbZ0pISKJKUMfX+rDoAAKg5fCiu+zNVcMv9yfGZj+/wIQBAdVWVKc+8Ean8/fkO1QAAqg4cQHWE+bFrfF6v1zJDipOQEsCfCBs3bsSCBQtwzTXXoFevXigtLcXcuXPx+uuvY+LEiYafSU9PR3q6cfyWFTeN+uOblhbX8YXGfaTur2ZTmnSNQcuxXxgfT40NUyCgaDb7w+IohIhwjJZCY7yeUDymjFN7BOHzSY9LEZpg3Jb4Pbb0+5Pjs4BGGSx8fsvPnYrfn2j8PRJx/B6l4viciK3KWF5eHjweDyoaLUlBKioqwqxlQV599VWceuqpGDVqFACgS5cuOHz4MP7f//t/OP/88+FxQoCh2ig8wTpjfhuyKSOWttAEdWsDu92Y6u03OcBdF8DvgPk0uwk9IakOsymjw6KvtmPrTHu9XnTv3h0bNmxQt/n9fmzYsAG9e/c2/ExtbW1YgKAjFDAtzW6HZGE2ZawK00ZFX6Pt35IxW1nRlbZwwIrRz8woQnQY9QcmTVBm2I7tbspx48bhySefRPfu3dGzZ0+88847qK2txfDhwwEATzzxBAoLCzF58mQAwPHHH4+3334b3bp1U92Ur776Ko4//njnKGWJVuA3Kjthdi2oWL0ptcqf05QHuzF57hVFCcyrEM5QblnAkRA9bIcUHcoM27FdGRs0aBCqqqrw2muvoaKiAl27dsW0adNUN2VZWZnOEjZhwgQoioJXXnkF+/fvR15eHo4//nhcfPHFdl96ZBKuwG/UDklT98sMguUVYhV9Da0z5sbGuWbPPdCkjDlhPilYCdGjhoo44Pl0InRT2o6UAP4xY8ZgzJgxhu/NmDFD9zotLQ2TJk3CpEmTbLiyZmJKo3Czi77G76ZUtEqIG1eKja2hTFVWFA+A8GQIKbCAIyF62JsyOlzA2Q6lsxkk3CjcQBCY3pInxsovVPlzs3Cyol2Qk9wgfgvGR0gq46Tn04lQGbMdSmczSDiA38AyZnbAZCK9KQF3Nwv3W+CmdFIjYmZTEqLHSc+nE2EAv+1wps0g4UbhgWkXtljGfMbvhz5sRgqiW7BCWXGSpZFNfwnRo8SIqXU7jTKRrY7sg9LZDBIN4DfKpjT7BzOmZYxuShWzy4poj+UES6OaoEDBSggAd3sC4sEKbwGJCmfaBIQawG9c9T+MaNmUpgfwRyptERJH5GrLmBVuymDRXQcot8yMIkRPrNI/bocyw3Y402bQWIFfidtNGSWb0izrRawK06EKiJstY1YIHifNJy1jhOhRHLRYciIM4LcdKmNm4GvsTZmwm9LIMhanQhfzHDFWfqFuylj7t2SsUFac5KZkzBghepz0fDoRKzLMSVQ402bQ3N6UwgbLWMTSFqFuShf3arNCWXGSG4SrXEL0uDksIx7M7tdLYkJlzAwSrsBvoCj5LcqmjOimDHnYaBkzV1lRY8YcIOxpGSNED3tTRoflcGyH0tkMEq3Ab2gZM7s3ZQxhE6r8uTiGQjQqK4rZ7ZACBzfvmM3FigQFQlIZo4x20gSzKW2HM20GzW4Uro0ZM3klEssMHxq0rioPEeqStWQsqcDvoJU3V7mE6HFSGIETYTal7XCmzSDhCvwGLiyza13FqqNDy1gTVtQZc1JpC8Z/EKLHzfIuHhhnajtUxswgwQr8ipGiZLYrKeGiry5eKZqdPAE4003JVS4hAZz0fDoRygzb4UybQcIB/EZFX002C2tcocJIwVKVv0YF0s0Vqc0uKwI4S7mlm5IQPU4KI3AilBm2Q2XMDBKuwB+lzphppS00xzFUxtgOScUSy5iDAoQZwE+InsZnXTgh29mJMLTBdiidzUCtM5ZEBf7Q9kTJoj1OVMtYsLSFg0ox2I0lMWPB79gByq3Z9xYhqY6TFktOhG5K2+FMJ4nw+Zoe6KQq8FuUTak9tpZIvSldaRmzMpvSAcKeLgdC9Li5rmI8qJYxqgh2wZlOlmDwPpBAnTG9FUoX02VazJjmhzfE2iWEiOymdKVlzAI3npMqfFth+SMklXFzWEY8MJvSdqiMJYloqG96kXAAf+OPpPYH2+x2SEC4wDFS/lxtGbOwtIUT5pMxY4TocdJiyYnQTWk7nOkkEfVay1icMWOhJnKtK8sON6VWQQhei5tTvVt8b0oWcCREBy1j0WFog+1QOidL0E2peKB44g3gDxEElrgpNccJDSLXKX8e/f5uFE5WmOQdFTNmcqYuIamOmxef8cBsStuhMpYkqpsyXhclEG4i1ypLZhd9BaK7KWkZ05S2aKGWMTYKJ0SPh27KqNBNaTuc6SQRiRZ8BcKtUHa7Kf0Gljgnte+xm5beDonBuITooZsyOlYsUElUONPJErSMxRsvBoRnLlrhpoxW9NVI+XOzcLIiwN1BjdcFLWOE6HFzWEY8hPYuJpZD6Zwkoj6ojMVZfR8IdwlaYBlTFCWy6zFqNqULzfZWuikdZBlTuMolBECjfATcKe/igW5K2+FMJ4lIsEk4gPB4IqMYLjOIlL7tN7KMuTiGwgo3npNW3qwzRogexUFhBE6E2ZS2Q2UsWZoTwB/6Q+03yG40g0gCR7saDG2H5ATlwW4sKW3hpGxKZkYRooOWsehwAWc7VMaSRK0zFm/1fSCqm1KxwzpDN6UeK+IjGo8lnKDc0uVAiB5mU0aHhaJthzOdLL5mKGOhmXZWtOMBosSMNa16lFA3pROUB7uxImbMSY3X6XIgRI/SGFbiRnkXDywUbTuc6SRpXp2xEBeWVZaLSBmSRpagxoK1wo0xFJY0CneQcmuVsk9IqsJG4dFhoWjboXROErXOWHMC+P0hbkqzLReeGNmUWuXD1W5KC+bfSQkRXOUSosfN8i4e/AxtsBvOdLKolrFESltEaIdk9iokUnkFI+XPSZYcu7EiwN1JddtYM4gQPcymjA5DG2yHyliSqG7KhGLGQhQfq4pyRsroM+o7FlqI1k1Y6qZ0wHxSsBKix0nPp8MQRglexHI400mSlJsyNJvS7B/LiJYxA+UjkkvTDVgZwO8Ey5hqeU3gHiWkJeNmT0AstAtyxozZBpWxZDGlzphVAfyxsymb9nWQ8mA3lsSMObDOGC1jhARwUraz09B1hKGKYBec6SQJWsaUhOqMRehNaXoAf4T0baPsOicFnNuNFW5KTwSrpAyM3NKEuBk3Lz5jYVVHGBIVKmNJoropEyptEcFNaVudsWjZlC4UTlaUfnDSfLLoKyF63ByWEQs/Y8ZkwJlOluYE8Idm8lhmGYugEBi5rZwU42Q3FrjxFLopCXEutIxFRuempMywCypjSdIUwN8cy1hQGbPKMhapN6WB8uck5cFGhGV9QR3k9mXRV0L0OOn5dBrMppQCZzpZmhXAH2Iit6oOlCpwfPrt/nDlT3GrcDJqmm4GTlp5001JiB4nPZ9Ow0/LmAwonZOkWaUtQl2ClrVDopsyJlbFR3gcZGm0IluUkFSGjcIjo1ugUkWwC850kiTXm1IECuxZ9WMZafXHdkhNWGYZc1IAP7MpCdHhpOfTaejclJQZdkFlLFlUy1gi7ZA0N7jwW9ibMlIF/milLdwlnCyLGXNSaQu6KQnR4yTLtdPQ/B4pVMZsg9I5SZpXgV8z7X6hqZAu0U3p1hgKq+IjnJQQYVWCCCGpilvlXTywl60UKJ2TpHluSq1lTGiUI5O/jkhVpg2zKd3qptQWOGyhdcYoXAnR41Z5Fw+0pEuBs50svmYUffWEuClVt6FF2ZQRLWPa3pTuXCkKbaapqaUtHNRuxSpln5BUJVLZH8KEH0lQOieJqG9O0VeNS1MI61YikZQxI+XPrb3a/BYFqzqpwjctY4TocZLl2mkw4UcKVMaSRDSnAn+oZcyqAP5I1hkjS4lbzfaa8SqmuikdZGlkBX5C9LC0RWRU40ACcdAkaaiMJYsviQr8QEAYWFbaIoKCZXQ+l2ZTNs2FVfF6DphPNUGEwpUQAM5KsHEatIxJgcpYkoj65jQK10y7lW7KSHFgUdshOUB5sBG1tIVl8XoOEPa0jBGix6UxsnHBsAYpUBlLlkbLmNLcbEq/JpvS7NIDkWrpGJXScJLyYCeWuYgdFJNilcJJSKriVnkXD8ymlAJnO0maYsYSqTMWGjNmdW9KZlNGxLIabw5yg1C4EqLHpZ6AuGA2pRQonZOkKZsy/gr8iqLohYFRRXwziFjaIpqb0gHKg51YFTPmkMbrQucGp3AlBIB7Y2TjgWENUqAyliy+ZlTgB/SlD6zOpoynHZKT2vfYiLBqFehxiJvSqqK2hKQyGvkrZD+jToMJP1KgdE6SZlXgB/SWE4vdlCKstIUv/HxujaGwqiBqMC1ctqDXfp90UxISIDSJijRBy5gUKJ2TpKk3ZaLKmMZNaXU7pEhFX43clC6zjFnmInZKHSOrem8Sksp4qIxFRLWMUV7YCZWxZGloRmkLQG8Zs7u8glFAt+stYy0zm1JnFaVwJSRAaBIVacLPhB8ZcLaTpFkV+AGdZUxYXWcsUjalrrSFO7OLLIsZc0pChPb7pHAlJADdlJGhm1IKlM7J0lw3pTbA22+gHJlBBAVLGMWoOamXop1YNvfOsIzp3JQM4CckQGitR9IES1tIgdI5SZofwK+xnFhdZyxSOyTDoq8uE0yWBfA7ZD51ljEKV0IAhFjGXLYAjQWzKaVAZSxJmh3A77EvmzKuOmNO6qVoJ36LglXV+fSZe9wEEdrz001JSADt8y47ycZp0E0pBUrnJBBCAKplLMFVhB3ZlJHillj0VUVVViwr+irbTUnLGCFhMGYsMiwSLQUqY8ng01gdEqjAD0DvQrTq5o9UXsGo6rxDKsbbjmVz75CECMHSFoSE4WE2ZUSsiqMlUUnQt2YOy5Ytw+LFi1FRUYHi4mJMmTIFPXv2jLj/wYMH8fLLL2P16tWorq5GUVERrrjiCgwYMMDGqzYgWH0fSLwCv7aul91B5EZmaJdmU1qfPOGQOmOKJ9CGixBCy1g0aBmTgu3K2GeffYZ58+Zh6tSp6NWrF95++23MmjULc+bMQZs2bcL2b2howMyZM5GXl4c//OEPKCwsRFlZGXJycuy+9HC0ylhz64zp3JTm3vyKJw0CMChtEaUdkmzlwW6sKivitAB+1hgjREVhnbHIMJtSCrYrY0uWLMGoUaMwYsQIAMDUqVPx1VdfYeXKlTjvvPPC9v/ggw9QXV2Nv//97/A2Kjzt2rWz85Ij06C1jCUawK+xnFjeKDw0ZiyKm1K28mAzTTFjVtUZk1301aLxEZLqKB69/CUBrFqgkqjYqow1NDRg27ZtOqXL4/GgX79++P777w0/s3btWvTq1QvPPfcc1qxZg7y8PAwePBjnnXcePBGUl/r6etTX16uvFUVBdna2+rdpBH/o0tIiXktEGi0VihAABAQAxWxXUuPDpAihO64iEDifR1G3K4onYEXT7Bv6b0tDURRdNqWZ41Q8wfn0S5s/RVF0grWlfY+uuD/B8VmGRwF84fLRLKSPr5kEfo0AeKLLjFQdn1OxVRmrqqqC3+9Hfn6+bnt+fj527dpl+Jndu3dj7969GDJkCP7yl7+gtLQUzz77LHw+HyZNmmT4mQULFuD1119XX3fr1g2zZ89GUVGRaWMBgAbFjxIAijcdHTp0SOizu7zp8AE4orAQdftbowJAVk4O2iZ4nGjsa5WDGgCtW7dGnua4B1o3ni+76XwHCgpQASA7MxNHhFxD+/btTbsmp3G49CcAQHpGJtqbOPfV+fkoB5CVkWHqd5ooDaW/AAgoh4neo6lCS74/AY7PKn5WPAB8aFdUBG87664h1b6/Qz/lowxARmYmjoxDZqTa+JyKlAD+RBBCIC8vD9dddx08Hg+6d++O/fv346233oqojI0fPx7jxo1TXwc1971796JB61pMlt2lgWtMS0NJSUlCH/U1+uX37d0LUVEBADhcW5vwcaKe4/BhAMCBykoc1BzXXxl+Pv+BAwCAQzU16jZFUdC+fXuUlpY2Ve1vQSiKgvxGl229z2fq3PsPVAEADh86ZOpxE0FRFLRt/N6Eoki7Dqtww/3J8VlIo3diz+5SKD7zzy99fM3Ev28fAKCuoSGqzLBrfF6v13RDihOxVRnLy8uDx+NBRaPyEaSioiLMWhYkPz8fXq9X5wbs2LEjKioq0NDQoMaRaUlPT0d6unGpCVNvGk1fyoSP2+hCFH4/hBrTpZh7fY1KqPD7dMcVvqY4ouB2geC+/rBrENr+mS0M4WsKVjVzjEL3/cqbO20dtRb7Hbbg+xPg+CxD84xaGduZct+fPzGZmHLjcyi2Ruh5vV50794dGzZsULf5/X5s2LABvXv3NvxMnz59UFpaCr+m/lVJSQkKCgoMFTFbaW4rJEAfMG9VFXi1fEakoq9GAfwuyyxq6e2QgvcW4zoI0eNWmRcLVuCXgu3pEuPGjcOKFSvw4YcfYufOnXj22WdRW1uL4cOHAwCeeOIJzJ8/X93/jDPOQHV1NV544QXs2rULX331FRYsWIDRo0fbfenh+JoC+BNGm01p1c0fq86YVvnzOER5sBujMh9m4JBsSsPvmhCir/VIVITfYLFOLMd209KgQYNQVVWF1157DRUVFejatSumTZumuinLysp02Rlt27bFnXfeiX//+9+47bbbUFhYiDPPPNOwDIbtqH0pE6y+D+gr3lt180eqAm9oGXOI8mA3PqtKWzhj1S2Mui0QQpxjvXYawqJC2CQqUvx8Y8aMwZgxYwzfmzFjRti23r17Y9asWRZfVeKIYNHXZrkpDXpT2lUF3sgt6tJ2SGqsg2WtqGRbxiyy/BGS6kRqF+d2WIFfCpTQyRBUxprlprShN2XMdki0jNmuCNsNi74SYoxTnlGnYVUcLYkKZzsZgm5Kb3PclAaWMavclJHaIWl/oD0uFUxWuYidYmlk/AchxkQK43A7ViWUkahQQidDMpYxRePGsiybMmgZ8+m3R2uHJFt5sBuLLEeKQwS9YGYUIcY4JK7TcdAyJgXOdjI0WsaURPtSAiHZlBa7KUMtY36DDDuXBrMKy7MpHWIZY8wYIXrcGpoRCzYKlwIldDIEM/GSqTPm92tufpvKK0R1U7pMMFk29w5RbmkZI8SYSItVt8OkHylwtpNBU4E/YQxjxszO6IuVTdnkXlXcarK3qg6XU+q2sbQFIca4VebFgtmUUqCEToZkSlsYZVOa7iqLlU2pdVO6swCiZXW4InU/sBnBoq+EGONxxjPqOBgzJgXOdjKoFfibbxkTVmZTJuKmdOsq0ar4CKckRNAyRogxjBkzpnE+FFrGbIUSOhnUCvxJxoxZXng0tDelwQ+0W2PGWnw7JMZ/EGKIx6UL0Fgw6UcKnO1kSMpNqfmxtry0RTyWMYcoD3ZjeV9Qp1jGuMolRIdbZV4sBAtFy4DKWBKIBjPqjPmtd1OGWsaMSlu4tDWI8FlccFd2nTEqY4QY45RQAqdBy5gUONvJ4DOpAr/VcUuh1hnDRuEOseTYTeN4TY+PcFxpCz7qhOhwyjPqNJhNKQVK6GQwowK/ruirTdmURsqfS7MpLYupckqmlp+ClRBDHJLx7Di4gJMCZzsZkgng17qxrGpWHawjFsky5qFlzDVFX+lyIESPU2oBOg1axqRACZ0MjZYxpVluSiPLmFXZlJHqjBkpY+4STMIofs4MnBIc7KcyRoghTmlZ5jQoM6TA2U6GZAL4PRq3oNW9KdkOKTJWZ1P6fdH3sxgG8BMSAbfKvFiwhZoUqIwlQzKlLbSWMatWIpFWflFLW7hslWjZ3DvE0kjBSogxbg3NiAXjTKVAZSwZkqjAryg2WsbCSls0XrdWAXFpaQvLkifUeD3ZyphF4yMk1WGjcGNYKFoKnO1kSKpRuDZmzOZ2SEYrH6fEONmM8FtU4NAp1b0Z/0GIMS6VeTFhNqUUONvJELSMNctNaUOdsUaFQCRSZ8xtljG/RZYjh6y6BTOjCDEmKB/dJvNiQZkhBSpjyZBUaQuDbEq7MvqMyh24dZVoVekHp8ynavnjo06IDrfGycaCljEpcLaTIakAfm2dseBKpBlZmdGIWNrCKJvSIQHndmN39wO7sarvKSGpDrMpjbGq3A+JCpWxJBA+EyxjfmHdzR8zm5JFX4XVmaySXSCCq1xCjHGpzIsJ3ZRSoIROhmTclOqPtc+6uJ5Y7ZA8DOC3LpvSIZZGBvATYoxD4jodBxuFS4GznQym1BkT1vnoIyhYhsqfU3op2o3dVkm74SqXEGPcugCNBa3pUuBsJ0MyFfgNY8ZsqgJv1I/RrSZ7izNZpQt6q0p3EJLqOKRLhuPgAk4KVMaSQe1NmWQ2pd0KgVFVdm0RWhdhWUyVQ+ZTWFW6g5BUhwH8xtAyJgXOdjKoAfzNaRRuYBmzq7yCYTskh1hy7MayuXeIpbHx/ApXuYTooZvSGPazlQKVsWQIuim9yTYKt2glEikOTFVANNftlIrxdmNZaQuHCHq2NiHEGLfKvFhYVfeSRIUSOhmSyqYMCgKfZbWglEjWrmhuStnKg90Yxc+ZgVM6Glg1PkJSHbd6A2LBfrZS4GwnQxKNwnVp1ZZnU4Y2CjdQ/pziVrMZYaSYmoFDVt2CBRwJMURxSFyn42A5HCk0Q4sgQTyjx6NVuhc1ua0T/7Cd2ZQRLWOah83jUsFkleBxSnCwVcomIamOSxegMWE2pRSojCWBZ9yFyO/QAYdKSppqd8WLVhBYXXg01FVmFCfllLpYdmOZItykjAkh5AXQ001JiDEO6ZLhOJhNKQXOtiy0ZSeCGW+2NQqP0g7JdZaxRlezVdmUgFzrGAP4CTHGKbUAnQYtY1KghJZFsCm4EBZm9EWwjBkpfw6JcbKbpjpcFinCgFxhzzR1QoxxqzcgFowzlQKVMVmoLkSfhW7KRCxjHs3bLlopCotixnSWMXnCXtBNSYgxtIwZw2xKKXC2ZaENrrcqiDzSys+ojoxDlAfbsapCvccpbkqucgkxhDFjxjCbUgqcbVlo06rtzqY07E2p+dtNcWNW9W7UzadEYW+VG5aQVId1xgwRjBmTApUxWdiRTRmxtEWUdkhG+7dghNVzD8i1NKqWsWZ0iSCkJUPLmDHMppQCZ1sWSng2pfmFR2O0QzKqMwa4zE1pcZ0xQKqlkatcQiLAmDFjKDOkQGVMFkFLhfBb1g4pYlFDI9ecU9xqdmOVIuyUbEqfRW5YQlIdZlMawwxsKVAZk4W2IKvV7ZBCLTNGAfxOCTi3G6vSuB3jpmRmFCGGOKVLhtNgbUIpcLZlYZRNaZWbMlI2pVHRV6P9WzCWlX5wSAyesKp0ByGpDtshGcOYMSlwtmVhlE1pVa2rSNmU2vM5xa1mNxbNvaIozhD2dDkQYoxbu47EwqqwGRIVKmOysCWbMlbR1wjZlG4STlYqK5E6INgJi74SYkwk+eh2aBmTAmdbFlpBYFk2ZQTLDN2UTVgpeCLF7NkJi74SYowTFktOhNmUUqAyJotGt5jQWcas6k0ZahkLV/4CbjX3rRSFldWmHRAgbFnvTUJSHZf2440JrelS4GzLQrsqs7sdUiTXnMeFK0UrV4FOiBljAD8hxijB8kLuWXzGRaPMUGhNtxVKaFloCw5abhmL1JvSY7y/m1aKas01C92UMoW9Vb03CUl1aBkzhuVwpMDZloX2h9qy8gqRAvgjWMacoDzYTaOyYskq0AnC3qrem4SkOuxNaYxRHUpiOVTGZKG1WllV2iJSzJI/hmXMRW5KYWkAv/zUect6bxKS6jghwcaJMGZMCpxtWdiRTRnJ7RjpB9rFljFLVoFOaLfCbEpCjHFjWEY8MJtSClTGZKGtjm+V9SJSI9xIyp8bG+daGjMm3zLGVS4hEXChJyAuKDOkwNmWhc5NaVV/xBi9KcMsY+5bKQorV4GR2lHZCeM/CDHGw2xKQ6zy1JCoUBmTha43pcXZlMKn3x5J+QsqD25aKVpZZ8wBbl9hpeWPkFTGCQk2ToSNwqXA2ZaFNrje6nZIoZaxSHXGHKA82I4dAfxSsykZ/0GIIcymNIYxY1KgMiYJRftDbVV/xEjChm7KJqzsTemEGDz2mSPEGGZTGmOlt4BEhLMtC60gsKw3ZaQK/JFKW7hPOAkr4yOcMJ+RvmtC3I4bF5/xQMuYFCihZWGUTRkMKDXtHAlmU7rRbO+3aO4BTZKGL/p+VsJgXEKMUVwYIxsPrE0oBc62LIyyKU13UzbFgAmtghXpYXNjQKulljH5yq2w0g1LSCrTKO+Emxaf8cCuHVKgMiYLxSCA3/TSFprj6ZQxBvCr+C0qKwI4I3We8R+EGOOEosxOhG5KKVBCy0LrQrSqmbP2eEaWsVAFxIVFEIWVBQ6dEJNCwUqIMU5IsHEiVoXNkKhQGZOF9ofa6nZIgF7BiqSAOKFIqd1YWVPHAW7KpppyfNQJ0eFGT0A8sIWaFCihZWEUPGpVOyRAfcCErq5ZBDeli7IprS1tIT9A2FLLHyGpjAs9AXHB2oRS8Mo46bJly7B48WJUVFSguLgYU6ZMQc+ePWN+btWqVXj00Udxwgkn4Pbbb7fhSi0k+EPt02TaWdUOCWhSwLSrwIh1xtyojLVQyxgD+AkxhpYxY5hNKQXbZ/uzzz7DvHnzMHHiRMyePRvFxcWYNWsWKisro35uz549+L//+z/86le/sulKLSb44+hr0Gwz2zKmVcYaf5S1gieszpgDYpxsxto6Yw5Qxlj0lRBjaBkzhtmUUrBdQi9ZsgSjRo3CiBEj0KlTJ0ydOhUZGRlYuXJlxM/4/X48/vjjuOCCC9CuXTsbr9ZCFDssY9qYsaBlzG/8PqBv0eQWrCyK6oQ6Ro3jUxj/QYgeN5byiQdaxqRgq5uyoaEB27Ztw3nnnadu83g86NevH77//vuIn3v99deRl5eHkSNHYtOmTTHPU19fj/r6evW1oijIzs5W/zaL4LGadUyDeCLFk2bq9WmzYRSEX6eS5tFv01hyFEVJbnwpgKIoqiBWPB7zx6kqQELKHCqK0mT5M/vecgCuuD/B8Vl2fk8aBABFWPN8yh5fs2lUxsJ+H0JI2fE5FFuVsaqqKvj9fuTn5+u25+fnY9euXYaf+e677/DBBx/ggQceiPs8CxYswOuvv66+7tatG2bPno2ioqJmXXcs2rdvn/Bnast3Yw8ADwSC6lj7Dh3gycwy7bqE34+djX8f2a4IaXn58B8+jF/U8x0FT1a2un9pRgbqARQWFCC7Qwd1e3PGlyr80qgMF7U7EumaMZvB7sws1AEozM/Xzaed7G60jBUWFkq7BqtpyfcnwPFZRU1hIfYByEhPRzsLn41U+/52AhAAio6MTyam2vicipQA/ng5dOgQHn/8cVx33XXIy8uL+3Pjx4/HuHHj1NdBzX3v3r1oaGiI9LGEURQF7du3R2lpacJVnMX+cgCAX2PBK929B0p6umnXp72m3SUlUA4egjhcoznfbigZmerrBl9AMdlfVgZPSUlS40sFFEVRsw337tsHJd08RRiAeq/t37cPnpISU48dD4qiQGmM/9hfUSnlGqzEDfcnx2cd/sY45drawyix4NmQPb7mIhplxt69ZVA8GRH3s2t8Xq/XMkOKk7BVGcvLy4PH40FFRYVue0VFRZi1DAB2796NvXv3Yvbs2eq24Jd+0UUXYc6cOYZaeXp6OtIjKDVW3DQitN1QPJ8x6FsoAgcz78KAgOtRiIDSIQSEr8ktKqCEZFc2BbRqx9Oc8aUMwZIfCqyZeyBsPm1Fjf9ouW1fWvT9CY7POoLPp7XnT7nvr9GaLhQlLpmYcuNzKLYqY16vF927d8eGDRtw0kknAQgE52/YsAFjxowJ2/+oo47CP/7xD922V155BYcPH8aVV16Jtm3b2nLdlqBmU1oYwB88jxDG2ZQRG4W7KKDVNaUtGIxLiA43yrt4EMymlIHtbspx48bhySefRPfu3dGzZ0+88847qK2txfDhwwEATzzxBAoLCzF58mRkZGSgS5cuus+3atUKAMK2pxxqNqWFpS2AQKKA32+cTRmaQejGbEpLS1s4oKMBK/ATYoynyXJNNFjVno9ExXZlbNCgQaiqqsJrr72GiooKdO3aFdOmTVPdlGVlZe7IzlAtY03KgCXjDi1sSMuYDmFpaQul8Rx+SLuj2ZuSEGOUxmxzNy0+48HKFnEkIlIC+MeMGWPolgSAGTNmRP3sTTfdZMEVSSC0xo1VP5ahCpaIovw5wa1mN1YWRXWApVGwgCMhxrBRuDFW/yYRQ6j6yiKsFZFFX0WoZSxa37HGfQV7U5qDE9yUVlr+CEllXOgJiAvVMkZlzE4ooWURqRWR6ecJtYxFiQdwo3AKKmNWCB41JsUJ7ZAoWAnRoXbIcNHiMwZCCFbglwRnWxZhrYgsdlMGlY5oliCDrgAtGV06tpIWecfm4oRGxLSMEWKMGxefsYjWLo9YCiW0LGS5KUUUS5ATlAc70WWWWtko3AHZlFzlEqKHjcLD0VoJKTNshbMti0jB82bjCTHFRzNBu61xrt/iVaADEiKElTFxhKQyDkiwcRzRsu2JpVAZk0WYm9Iqy1iEbEqj87nOMmbxKlBxgNuXRV8JMcZt8i4eotWhJJbC2ZZFWAC/E7IpXWa291sseJxgaWRmFCHGOOH5dBpWL1BJRDjbsgiLGbOrzlg82ZQuWSlabJJXnLDyZswYIcY44fl0GlbH0ZKIUELLIvRGt7y0RUgAf5Q6Y64RTrqYMQuLvkpMnRdWlu4gJJVxmycgHvyMGZMFlTFZhP74WxYzFhK3xNIWTVgdrOqEbErGjBFijBOKMjsNYfEClUSEsy0Lu7IpQ1d/UfqOKU5QHuzE6mBVJ7h9WfSVEGOcUJTZadAyJg0qY7Kwrc5YAm5Kt6V6W24Zc8DKm0VfCTHGbWEZ8aCRVQplhq1wtmVhW8xYY2V5EVJnzLC0hcssYxqXbVjTdDNwgGVMMICfEGOYTRkOWyFJgzMuC1l1xvxxZFO6xWxvtQvP44AA4WilTAhxM7SMhROtQwuxFCpjsrCrUXgibkq3CSerXXhBq6RUZcwX+JfKGCF63OYJiAcu3qRBZUwWdsWMhWZIiigPm9uEk9UuPAe4KaO6pQlxM2qmuUsWn/HAsAZpcMZlEeamtKvoa5SHzQluNTuxum+jA5QxwdIWhBjjtsVnPLCXrTQooWVhdzZlcPUXrQhoaLB/S8fqYFUnZFMyBoQQY1jaIhxa0qXBGZeFbXXGQuLA4mqH5JKVotWCxwGWMRZ9JSQCTlgsOQ1mU0qDMy4L20pbhAgc1TLGbErLg9slt0MSQkSPESTEzbitrmI8sEi0NKiMycK2dkih2ZTRAvhdJpxUy5jVMWM+a44fC11RWz7qhOhwmycgHqyWiSQilNCSUBRFrxBZnU0pmE0ZhjoXadYcX7alUauMMQaEED1uW3zGA8MapMEZl4lOGbO6N2VonTGjbEqXCadoyQxmIHs+dU1/udIlRIfsxZIToZtSGlTGZKJViBzhpnSbZczq0haSA4S1JUrodiBEj6YdknDLAjQW7GUrDc64TGyxjAW+YrXeVLQA/tACsS2daK2hzEB2NiVjxgiJjPaZoDIWgNmU0uCMy8Rjo5syrOhrNMuYSwSTiKKYmoHsmBS6KQmJjIfKWBh0U0qDyphMtIHjlgfwB4u+xpFN6ZYYCqvLPsjuaOBnAD8hEdE+924JzYgFS+FIgxJaJlrLmOXWmZBsymh1xtwimKzOHJI9n3RTEhIZuinDiRbGQiyFMy4TO2LGQlt+RMumlO1WsxurWwXJtjT6NfXNuNIlRI/2mXCLNyAWdFNKg8qYTGzJptQrWCKaGdrjMsuY1cGqoTXe7EZnGaNwJUSHzjImqTCz07A6qYlEhDMuEzvrjCXSDsklljGhpnFbPfeSsykVJVBkmBDShIeWsTDYKFwanHGZ2Fr0NZ5sSpeVtojmsjUD2XXGrM4WJSSVYcxYOHRTSoNSWiYeG7IpQ+PAomVTyq4Ybzd+iwVPaLye3UT7rglxOx5mU4bBbEppUBmTiZ0B/GG9KaO4KV1jGbPYJC/dTck+c4REhJaxcCgzpMEZl4kNpS2UUMtYNNeVW7MpW6qb0mrLHyEpjMI6Y+FY3a+XRITKmEy0SoDVGX1qaQtmU6pYrazItjSytQkh0ZFdfsZpUGZIgzMuE60SYHlGX2gAP7MpLXdTyo7BY2YUIdGR3SXDabBRuDQ44zKxtbRFSAC/kfLntlWiTW5KITubkm5KQoxx2wI0FpQZ0qAyJhNNNqViV9xSNDel7PY9dtPS64wx/oOQ6MguzOw0mE0pDSpjMpFRZ6zxX0Plz7V1xlpoaQvGfxASHbclLcWC2ZTS4IzLRKeMWRW3FGKdiaaAeIJuNZcIpkYlyTarpN0wm5KQ6LjNGxALWtOlQWVMJtogSatu/qAr1B/qpowWwO8SwWR1hXrZbkoG8BMSHbfFycZA0JouDc64TOywjIUqBNEUkFArWkvH6vgI2W5fuhwIiY7bFqCxiNa7mFgKZ1wmtpa2iKPOmNviJ/wWKyuyg4PppiQkOrLjOp0GA/ilQWVMJrb2pgwG8EcrbeGymjuucVNSsBJiiOy4TqdBZUwaVMZkIqPOWNSiry6zjFnupnSIMkY3JSHGyC7M7DQY2iANzrhMbMmmDIlbiua6cptlzOr4iNC5txvGfxASHcaM6WFogzQopWViRzZlWDukKNYS2TFOdmNxnTHFMZYxClZCDHGbNyAWzMCWBmdcJrYWfQ3NpoxWgd8lgsnqPmyS41GE1QkKhKQ6ofLR7dBNKQ3OuEzsdFOGZVNGs4y5RDDZFTMmvbQFLWOEGKI+oz651+EUrG4RRyJCZUwm2mxKu6wzUaxBitviJ4IC2K7uB3bDAH5CouO2BWgsGNogDUppmUjNpoxSZ8wtJnvLLWNp+vPYjdWlOwhJdRgzpkdE6V1MLIUzLhNbKvBHclNGixlziWXM6mBVj2w3JVe5hETF4zKZFwtmU0qDyphMtAqYVTd/qEIQLUBTtlvNbqwOVpWdEMGmv4REh5YxPQxtkAZnXCbaH0m7SltErTMmuS6W3Vi9CpRd3dtPwUpIVGQn2TiNaNn2xFIopWWis4xZ7Kb0h7gpjR42twWzWu2mlG0ZY8wYIdFxW2hGLGgZkwZnXCYeO9yUIdaZaA+b2wST1RXqZa+6GTNGSHTclrQUC5a2kAaVMZlofyTtss5EU0DcJpisVlZkWxpZwJGQ6LgtTjYWlBnS4IzLxNZsylDLGLMpRYuPGWNmFCFRkf2MOg1a06VBZUwmtmZThtYZi+amdMkq0epVYOjc203j96gwZowQY2Rbr50GW6hJgzMuEUVnGbOr6GuUlY/bBJPlAfySV910ORASHZd5A2LCbEppUErLRKsEWK0Q+OMpbeGyNG+7elNKqzPGYFxCosJG4XqYTSkNzrhMbLWMhRR9NVL+QjMvWzpWm+RlF5Rk/Ach0ZFtvXYaUXoXE2vhjMtEV9rCJoXAHy2A321uSotN8rJbrTD+g5DoND6jgpaxANF6FxNL8co46bJly7B48WJUVFSguLgYU6ZMQc+ePQ33ff/99/Hxxx/j559/BgB0794dF198ccT9Uwo7Sls0HleEZlMalrZwmZvS6gr1sueTgpWQ6NAypofWdGnYvmT+7LPPMG/ePEycOBGzZ89GcXExZs2ahcrKSsP9v/32WwwePBjTp0/HzJkzccQRR2DmzJnYv3+/zVduAXZkU4YF8EfLpnSrZcxaRVi6m5IuB0KMkf2MOg1a06Vhu2VsyZIlGDVqFEaMGAEAmDp1Kr766iusXLkS5513Xtj+N998s+719ddfjy+++ALffPMNhg0bZniO+vp61NfXq68VRUF2drb6t1kEj9XsY2p+JBWPx9Rr0x5XAFCECBxfU+4g7HwawaQoSvLjcziKEIG5UayZe3jSAv/6hZQ5FBplrCV+hy3+/uT4bLkGnXw0+djaf1OCBGRGSo7PwdiqjDU0NGDbtm06pcvj8aBfv374/vvv4zpGbW0tGhoakJubG3GfBQsW4PXXX1dfd+vWDbNnz0ZRUVGzrz0a7du3b9bn9rdqhYONf+fl56N1hw7mXVQj1QUFKAeQmZGBog4dUJaViUMA8tq0CTtfXU0ldgNI8yjooHmvueNzOuXZWagGkJuXhzYWzH19/SGUIhCW0sGC48fiQG4uKgBkZWejrYTz20VLvT+DcHzWsTcrG4cB5LfJQyuLnpFU+v7Kc7JVmZgf53yk0vicjK3KWFVVFfx+P/Lz83Xb8/PzsWvXrriO8dJLL6GwsBD9+vWLuM/48eMxbtw49XVQc9+7dy8aGhoSv/AIKIqC9u3bo7S0tMkKkQC+Q4fUv6uqDqC6pMS0awvir6oCANQeqkFJSQl8NTWB8x2oDjuf2Bdw/frq61FSUpL0+JyOvzqgClcfPIgaC+ZelJUFzuPzocSC48fCX1kBADhcWyfl/FbT0u9Pjs96fHV1AICK8nJUmfyMOGF8ieKrrgYQkImHYsyHXePzer2WGVKchJQA/uaycOFCrFq1CjNmzEBGRkbE/dLT05Genm74nhU3jRCiecfVuCmFolhzbWjKFhJCqC2AhBI+F+qrkPE0e3wOR/h9gT8snnsIv5z509SUa4nfX5CWen8G4fgsPHfjQl34fJZdQ0p9f82QGSk1Pgdja5ReXl4ePB4PKioqdNsrKirCrGWhvPXWW1i4cCH++te/ori42LqLtBMZdcaiZRC6rc6Y5Y3CJRd9ZQA/IdFhAL8eZlNKw1Yp7fV60b17d2zYsEHd5vf7sWHDBvTu3Tvi5xYtWoQ33ngD06ZNQ48ePey4VHuwpTdliLCJlkGoVut3iWCyqx2StNIWFKyERMVtGeSxYDalNGyf8XHjxmHFihX48MMPsXPnTjz77LOora3F8OHDAQBPPPEE5s+fr+6/cOFCvPrqq7jhhhvQrl07VFRUoKKiAocPH7b70s1HV2cszdpzxNObUnb7HruxWvDItjRSsBISHdmFmZ0GaxNKw/aYsUGDBqGqqgqvvfYaKioq0LVrV0ybNk11U5aVlelSZZcvX46GhgY8/PDDuuNMnDgRF1xwgZ2Xbj52Wsb8IUVfDd2ULhNMVlfgl933joKVkOi4bQEaC1rTpSElgH/MmDEYM2aM4XszZszQvX7yySdtuCJJaJUAixSCYB2d8KKvbIdkfQV+Fn0lxMkoSqAOo2tCM2KhLlAt8tSQiFBKy0RnGbNaIQgJ4DdS/kKD/Vs6Vlfglz2ffovHR0iqoz6jPrnX4RSi/T4QS6GUlonHxmxK1U0ZRzskt/SmdEs2JV0OhBjjtqSlWFBmSIPKmEzssIyFKgT+aG5Kl8VPWB3grnFTyqjDI6J914QQ+Qsmp8GkH2lwxmWi+ZFULAsiDy1tEUedMbesEq0OcNceV4arMpoVlBASHsbhdpj0Iw1KaZnYYhkLzab0N56O2ZS21RkD5Ci4dDkQEh1axvREW6wTS+GMy0SxPpsysTpjmvZMbhBOlseMaR4vGfPJbEpCouO2ONkYCKuTmkhEOOMy8dhgGQs1w4s4sim1+7dkgr0prc6mBOTMJ+M/CImO2+JkY0FrujQopWVia2/K0AD+KNmUgDvixvwWCx5FtmXM4qK2hKQ6tIzpYWkLaVAZk4ktlrGQ0hbxZFMC7rCMWe3G88i2jDH+g5CoMGZMD5N+pMEZl4kdlrGwRuFxtEMC3GEZs1rwKJLnk5lRhESH2ZR6WA5HGlTGZKJVAiwP4A8t+mpkGdO0wHCDcLJa8Eh3UzKAn5CohC5W3Q6zKaXBGZeJzjJmdWmLEMtYtNIW2v1aMlYLHp1lTEadMQbjEhIV2S3LnAazKaXBGZeJ9oa3utZVaDYlY8YsT+PW1XKTEjPmC16I/ecmJBVQY2pdsPiMBy7gpEEpLRNd0VebsymNFBDZbjW7sUPwhHZAsJPGc1rW3YGQVIcxY3qojEmDyphMPDa4KSO2Q4phGXPDStFvQ+kHmV0NGP9BSHRoGdMTbbFOLIUzLhNbsilDA/ij/EC7zE3ZNBdp0fdLBpnCnoKVkOi4rQVcLILt8mgZsx1KaZnY6aZU64z59Nt1uyrh+7dk7LCMOcBNSZcDIREILsQkPJ+i5iD8b78GsafE9nNHhLUJpcEZl4muN6UD3JTa7YwZMweZ2Vpsh0RIdCRaxsR/V0IsfBHirfm2nzsizKaUBmdcJh4bLGNqaYvQ3pQRvnqPiwJa7VBWZLZbYdFXQqIjc/G5f2/g1KW/2H/uSNCaLg0qYzLRFX21uNaVahmLsfKR6VazGztWgTKLSlKwEhIddbEk4fmsqgj8u9dBbkom/UiDMy4TO4q+hipXsZpjuylmrKW7KVmBn5DoSHw+RVAZqzkIcfCA7ec3xI44WmIIpbRMdKUt7GqHFGPl4ybLmB1uSpmNiJlNSUh0ZC4+g8oYAOwptf/8RjC0QRqU0jLRZVNa1Q4pgpsy0sMmU3mwm2Aatx3ZlGwUTojz8MjLpkRVpfqncIqrktmU0uCMy0SXTWlxs2q1tEUMa5Ar3ZRWBvDLzKakYCUkKpKyKYXfDxyoaNqwl5Yxt0MpLRHFjmzKSG7KSMqfm9qD+G2IqZJaZ4xuSkKiIiub8uAB/YLXKZYxxplKgzMuEzvclKFusphuShfFjInIBXBNQ6alkdmUhERHVhiBNl4MgHCMZYzWdFlwxmViSzukCEVfI5a2cFF7kFiZpWYgs26bsCFBgZBURpa8CypjwfMzgN/1UErLROsqtLzOWDBmLM5sSjc0zrXDJC+zqKSfgpWQqEgqyqyWtehYHPi3Yh9EXa2t12AIS1tIg8qYTJyYTemqdkh2lLaQWYE/RnwgIW6n8dkQdsu7RmVM6dAZyM4JbCvbbe81GEE3pTQ44zKxw02pCSAXQsR+2CT2arMdOyxHUi1jDMYlJCqyEpaClrG8fKCofeBvJ8SN0ZouDUppmWh/JC0rbaE5rvDHYRmTaMmxEeHzAQerAy9yWll3IidkU3KVS4gxsuoqapQxpahD4BKckFHJbEppcMZlYmc2JRCwlMSKCXCLm7JsN+BrgJKZCRS0te48TmiHxFUuIcZIWiyJqvLAH3n5QLtGy5gTgvjpppSGV/YFuBpb3JRay1gcbkq3ZFOW/gIA8B5VDOHxWBczIlG5FXa0eyIklZFVeiYYM5aXr8oeR5S3YDalNKiMycSORuFac7PfH9taEmwP0sKzKcXunQAAb6di1Ft5IqkB/BSshETFATFjSkYmBOCMmDEm/UiDyphMdDFjNrgpg0VOo53PLW7KRstYekeLlTGZHQ0Y/0FIdFTLmH3yLtAKqbEvZV4B0Do/8HfZbgi/D0pwQSwDWtOlwRmXiS1FXzXH9WkUApe7KcXuRjdlp2JrT+SxX9irMP6DkOjIyB6vqQZ8jQvjvDZAQSHg9QK+BqB8n33XYQSt6dKglJaJYnM2pd9nvF2LW9ohBWPGOnax9jwysymD3zddDoQYI+P5DLooc3KheNMDlrC2Rwa27ZGcUclyONLgjMvEY3M2pU/rpoyVTdlyLWOi5qAqENOttozJnE872j0RksrIWHxqa4wFCZa3aFwkSoPWdGlwxmVidzalVhmL5aZsyXXGdu8K/NumAJ6cXGvPJTMGj4KVkKgoEhZLwkAZU3r0Dby3+mPbrsMQuimlQSktE1vqjGmVsQbj7UbX1ILdlMFMShzZ0fqTOaFROF0OhBgjo8i1pqyFehmDTws8p1u/hdi5w75rCYW1CaVBKS0Tj/WWMUVRNNauOCxjaimGlquMBePFlPY2KGMyLY0UrIRER4blWlvwNXgZ+YVA/1MCl/LRMvuuJRQ/F3Cy4IzLRPsjaeXNHzy2NpsyZmmLFuymVJWxTtafS2ZCBNPUCYmOR8JiyShmDIBn+JkAAPHflRCHD9l3PVroppQGpbRMdG5KG5pVM5sSQFNZC9hiGQvMp5BS2sLfeAkUrIQYojTW9LJR3omqYI2xfP0bffoB7Y4CDh+CWP2Rbdejg3Gm0uCMyySo+ChKUyCpFahxEXEoYy3cMib8fmBPIIBfsSNmjL0pCXEuMhqFqzFjBbrNiscDZdiYwOV8uNS6Fm3RYGkLaXDGZRL8kbT6xzJ4/GA2ZTTlr6VX4C8vA+rqgDRvU20fK5E5n6pglVjRmxAnI2OxFMFNCQDK4FGANx34eTvEklftV8joppQGlTGZBC1WVpuEg8fXKGOx9hUttbRFsI5PUXsoaTYoKVLbIVGwEhIVxd6EJSEEcKAi8MJIGWvVGsq4CwP7vjUf4vk5EPWWNmzTX5t6IVQN7Ia9KWVil2UsaIoPlraI9qC18DpjtsaLAXLbIfmpjBESFbstYzUHgYZGOZzXxnAXz9gL4M/Ng5j/dCCY/+dtUE4cCuXYk4COxdaFtGhlPuNMbYfKmEyCN7zVN37jw+t/b6HutfE1NVrGPnkPvs3fYH9OK/gO1WgPBgQ/rihQXyjQ/G2TktkMxNZNAGyKFwOaLI0v/RO+ZW8ArdsA6emANx2KNx1Izwi81iVzaD+ve2G8T9h7jX8fPKC7BkJICMFnpXwf/K/8y/TDl7dqBd/Bg00bgrI0uxWU9IyIn/MMGwNRdCT8T88GfvkR4pcfIRa+GJAX+YWB/zKzAa83IEe86YH+lmlpccoMA3mhVUgdKLtbOlTGZJKVE7jpsy2uAt+qdWBF9s2axteRz6e0ag0BAJu/gdj8DQ5G3DPFsboNUiPKrwdAbFgL1NUCJT8H/mvENltZdo5dZyIktQh24Kiphlix2PTDV0d6o227mJ9Vjj4OnplPQ6z7L8TXq4Hv/gfU1wF7SwP/NWK6HAkuEomtUBmTiJKXD891fzaMHTATz/V3QGz8KmCGFn4ov+of+ZomXQV07QU01ENRFLTOzcWBAwcggqsmAU0wumiUBI2vhdBIBgcnAOTmQTlhiC2n8owaBzHktEDiwP4yiIMHgPp6oKEOqG9o/Le+aU7DAnaF4Z/67RH2UYCCvr9B1ZFHycnMIsTpdOkO5fLfAmW7LTl8bm4uqqtDVTIFyvED4/q8kpcfyLAcNiYQO1axDyjfB1G5P5CI1FAfcHs21Af+U1veNV9uKD37RrXaEWugMiYZ5fhB1p+jS3coXbrHt29hEZTR4wN/KwryOnTAwZIS/pgngZKZBbTvBLTvFO5dtPK8ioJWHTqgqqTExrMSkjooigJl6BmWHTu/QwccMkl+KunpQFH7QPKRCddHnAWDSQghhBBCJEJljBBCCCFEIlTGCCGEEEIkQmWMEEIIIUQiVMYIIYQQQiRCZYwQQgghRCJUxgghhBBCJEJljBBCCCFEIlTGCCGEEEIkQmWMEEIIIUQiVMYIIYQQQiRCZYwQQgghRCJUxgghhBBCJOKVfQF24vVaM1yrjusUOL7UhuNLbTi+1Ibjc/bxnYIihBCyL4IQQgghxK3QTZkEhw4dwp///GccOnRI9qVYAseX2nB8qQ3Hl9pwfCQRqIwlgRAC27dvR0s1LnJ8qQ3Hl9pwfKkNx0cSgcoYIYQQQohEqIwRQgghhEiEylgSpKenY+LEiUhPT5d9KZbA8aU2HF9qw/GlNhwfSQRmUxJCCCGESISWMUIIIYQQiVAZI4QQQgiRCJUxQgghhBCJUBkjhBBCCJEIlTFCCCGEEIm4owOnRSxbtgyLFy9GRUUFiouLMWXKFPTs2VP2ZSXEggULsHr1avzyyy/IyMhA7969cemll+Koo45S96mrq8O8efPw2Wefob6+HsceeyyuueYa5Ofny7vwZrJw4ULMnz8fZ511Fq688koAqT++/fv348UXX8T69etRW1uL9u3b48Ybb0SPHj0ABCplv/baa1ixYgUOHjyIvn374pprrkGHDh0kX3ls/H4/XnvtNXzyySeoqKhAYWEhhg0bhgkTJkBRFACpN75vv/0Wb731FrZv347y8nL86U9/wkknnaS+H894qqur8fzzz2Pt2rVQFAUnn3wyrrrqKmRlZckYko5o42toaMArr7yCdevWYc+ePcjJyUG/fv0wefJkFBYWqsdI1fGF8v/+3//D+++/jyuuuAJjx45Vt6f6+Hbu3ImXXnoJ3377Lfx+Pzp16oQ//vGPaNu2LYDUl6kyoGWsmXz22WeYN28eJk6ciNmzZ6O4uBizZs1CZWWl7EtLiG+//RajR4/GrFmz8Ne//hU+nw8zZ87E4cOH1X3+/e9/Y+3atfjDH/6Ae+65B+Xl5XjooYckXnXz2Lp1K5YvX47i4mLd9lQeX3V1Ne666y54vV5MmzYNjzzyCC6//HK0atVK3WfRokVYunQppk6dinvvvReZmZmYNWsW6urqJF55fCxcuBDLly/H1VdfjUceeQSXXHIJ3nrrLSxdulTdJ9XGV1tbi65du+Lqq682fD+e8Tz22GP4+eef8de//hV33HEHNm3ahGeeecauIUQl2vjq6uqwfft2TJgwAbNnz8Yf//hH7Nq1Cw888IBuv1Qdn5bVq1djy5YtKCgoCHsvlcdXWlqKu+++Gx07dsSMGTPw4IMPYsKECbp6Y6ksU6UhSLP4y1/+Ip599ln1tc/nE9dee61YsGCBvIsygcrKSjFp0iSxceNGIYQQBw8eFBdddJH4/PPP1X127twpJk2aJDZv3izrMhPm0KFD4uabbxZff/21mD59upg7d64QIvXH9+KLL4q77ror4vt+v19MnTpVLFq0SN128OBBMXnyZPHpp5/acYlJcd9994mnnnpKt+3BBx8Ujz76qBAi9cc3adIk8cUXX6iv4xnPzz//LCZNmiS2bt2q7rNu3TpxwQUXiH379tl38XEQOj4jtmzZIiZNmiT27t0rhGgZ49u3b5+47rrrxE8//SRuvPFGsWTJEvW9VB/fI488Ih577LGIn0l1mSoLWsaaQUNDA7Zt24Z+/fqp2zweD/r164fvv/9e4pUlT01NDQAgNzcXALBt2zb4fD7dWDt27Ii2bdum1FifffZZHHfccTjmmGN021N9fGvWrEH37t3x8MMP45prrsHtt9+O999/X31/z549qKio0I07JycHPXv2TInx9e7dGxs2bMCuXbsAADt27MDmzZtx3HHHAUj98YUSz3i+//57tGrVSnVDA0C/fv2gKAq2bt1q+zUnS01NDRRFQU5ODoDUH5/f78fjjz+Oc845B507dw57P5XH5/f78dVXX6FDhw6YNWsWrrnmGkybNg2rV69W90l1mSoLxow1g6qqKvj9/jD/d35+vvqjkYr4/X688MIL6NOnD7p06QIAqKiogNfr1bm9AKBNmzaoqKiQcJWJs2rVKmzfvh333Xdf2HupPr49e/Zg+fLlGDt2LMaPH48ffvgBc+fOhdfrxfDhw9UxtGnTRve5VBnfeeedh0OHDuH3v/89PB4P/H4/LrroIgwdOhQAUn58ocQznoqKCuTl5eneT0tLQ25ubsqNua6uDi+99BIGDx6sKmOpPr5FixYhLS0NZ555puH7qTy+qqoqHD58GIsWLcKFF16ISy65BOvXr8dDDz2E6dOn4+ijj055mSoLKmNE5bnnnsPPP/+Mv/3tb7IvxTTKysrwwgsv4K9//SsyMjJkX47p+P1+9OjRA5MnTwYAdOvWDT/99BOWL1+O4cOHy704E/j888/x6aef4uabb0bnzp2xY8cOvPDCCygoKGgR43MzDQ0NeOSRRwAA11xzjeSrMYdt27bhnXfewezZs9UEk5aE3+8HAJxwwgkYN24cAKBr167YvHkz3nvvPRx99NEyLy+loTLWDPLy8uDxeMK0/IqKipTNFnnuuefw1Vdf4Z577sERRxyhbs/Pz0dDQwMOHjyoW+lUVlamxFi3bduGyspK/PnPf1a3+f1+bNq0CcuWLcOdd96Z0uMrKChAp06ddNs6deqEL774AgDUMVRWVuoCiSsrK9G1a1e7LrPZvPjiizj33HMxePBgAECXLl2wd+9eLFy4EMOHD0/58YUSz3jy8/NRVVWl+5zP50N1dXVK3LNAkyJWVlaGu+++W7WKAak9vk2bNqGqqgo33nijus3v92PevHl455138OSTT6b0+PLy8pCWlhYmczp27IjNmzcDSP3fDFlQGWsGXq8X3bt3x4YNG9SUX7/fjw0bNmDMmDGSry4xhBB4/vnnsXr1asyYMQPt2rXTvd+9e3ekpaXhm2++wSmnnAIA2LVrF8rKytC7d28Zl5wQ/fr1wz/+8Q/dtn/+85846qijcO6556Jt27YpPb4+ffqEucZ37dqFoqIiAEC7du2Qn5+Pb775Rv0xr6mpwdatW3HGGWfYfbkJU1tbC49HH9rq8XgghACQ+uMLJZ7x9O7dGwcPHsS2bdvQvXt3AMCGDRsghEiJ0jpBRay0tBTTp09H69atde+n8vhOPfVUXawUAMyaNQunnnoqRowYASC1x+f1etGjR48wmVNSUqKWtUj13wxZUBlrJuPGjcOTTz6J7t27o2fPnnjnnXdQW1ubcq6T5557Dp9++iluv/12ZGdnq9a+nJwcZGRkICcnByNHjsS8efOQm5uLnJwcPP/88+jdu3dKPFjZ2dlq/FuQzMxMtG7dWt2eyuMbO3Ys7rrrLrz55psYNGgQtm7dihUrVuDaa68FACiKgrPOOgtvvvkmOnTogHbt2uGVV15BQUEBTjzxRMlXH5vjjz8eb775Jtq2bYtOnTphx44dWLJkifrDlorjO3z4MEpLS9XXe/bswY4dO5Cbm4u2bdvGHE+nTp3Qv39/PPPMM5g6dSoaGhrw/PPPY9CgQbpaXbKINr78/Hw8/PDD2L59O/785z/D7/erMic3Nxderzelx9e2bdsw5dLr9SI/P1+t3Zjq4zvnnHPwyCOP4Fe/+hV+85vfYP369Vi7di1mzJgBACn/myELRQSXmCRhli1bhrfeegsVFRXo2rUrrrrqKvTq1Uv2ZSXEBRdcYLj9xhtvVBXLYAG/VatWoaGhIeUL+M2YMQNdu3YNK/qaquNbu3Yt5s+fj9LSUrRr1w5jx47Faaedpr4vGouIvv/++6ipqUHfvn1x9dVX6wr7OpVDhw7h1VdfxerVq1FZWYnCwkIMHjwYEydOhNcbWEum2vg2btyIe+65J2z7sGHDcNNNN8U1nurqajz33HO6oqFTpkxxRNHQaOObNGkSfvvb3xp+bvr06fj1r38NIHXHd9NNN4Vtv+mmm3DWWWeFFX1N5fF98MEHWLhwIfbt24ejjjoKF1xwgW7xk+oyVQZUxgghhBBCJMI6Y4QQQgghEqEyRgghhBAiESpjhBBCCCESoTJGCCGEECIRKmOEEEIIIRKhMkYIIYQQIhEqY4QQQgghEqEyRgghhBAiESpjhBBCCCESoTJGCCGEECIRKmOEEEIIIRL5/yNcwzZKv8xkAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSnZCtdTGFL5"
      },
      "source": [
        "def BP_QL(num_episodes, prior_dist, discount_factor = .6, alpha = 0.6, epsilon = 0.9):\n",
        "    \"\"\"\n",
        "    Muti-agent RUQL algorithm. Designed based on above Q-Learning script from GeeksForGeeks\n",
        "    \"\"\"\n",
        "\n",
        "    #Creates lists corresponding to each agent's Q tables, stats, and policy functions\n",
        "    Q = []\n",
        "    policy = []\n",
        "\n",
        "    #Loop through each agent in the environment\n",
        "    for i in range(2):\n",
        "\n",
        "      #Generate a new Q table for each agent\n",
        "      Q.append(defaultdict(lambda: np.zeros(2)))\n",
        "\n",
        "      # Create an epsilon greedy policy function for each agent\n",
        "      # appropriately for environment action space\n",
        "      policy.append(createEpsilonGreedyPolicy(Q[i], epsilon, 2))\n",
        "\n",
        "    states = [np.random.choice([0, 1], p=prior_dist), 0]\n",
        "    next_states = states\n",
        "    index = 0\n",
        "    actions = [0, 0]\n",
        "    for ith_episode in range(num_episodes):\n",
        "        #print(\"On round: \", ith_episode)\n",
        "        if ith_episode > 0:\n",
        "          alpha = 1/(ith_episode)\n",
        "        #loops through each agent\n",
        "        if ith_episode % 2 == 0:\n",
        "          index = 0\n",
        "        else:\n",
        "          index = 1\n",
        "          states[1] = actions[0]\n",
        "\n",
        "        policy[index] = createEpsilonGreedyPolicy(Q[index], epsilon, 2)\n",
        "\n",
        "        action_probabilities = policy[index](states[index])\n",
        "        if index == 0:\n",
        "          print(\"Prosecutor action probabilities: \", action_probabilities)\n",
        "\n",
        "        action = np.random.choice(np.arange(\n",
        "                  len(action_probabilities)),\n",
        "                  p = action_probabilities)\n",
        "\n",
        "\n",
        "        if index == 0:\n",
        "          print(\"Prosecutor took action \", action, \" on episode \", ith_episode)\n",
        "\n",
        "        actions[index] = action\n",
        "\n",
        "        if index == 1:\n",
        "          #pass action probabilities for each agent into env.step, which will update agent states state and environments by picking certain action given probabilities\n",
        "          rewards = reward(actions[0], actions[1])\n",
        "          #choose action given probabilities for each agent\n",
        "\n",
        "          for i in range(2):\n",
        "            action = actions[i]\n",
        "            if i == 0:\n",
        "              next_states[0] = np.random.choice([0, 1], p=prior_dist)\n",
        "            else:\n",
        "              action_probabilities = policy[0](next_states[0])\n",
        "              action = np.random.choice(np.arange(\n",
        "                        len(action_probabilities)),\n",
        "                        p = action_probabilities)\n",
        "              next_states[1] = action\n",
        "\n",
        "            best_next_action = np.argmax(Q[i][next_states[i]])\n",
        "            td_target = rewards[i] + discount_factor * Q[i][next_states[i]][best_next_action]\n",
        "            td_delta = td_target - Q[i][states[i]][action]\n",
        "            Q[i][states[i]][action] += alpha * td_delta\n",
        "\n",
        "          #print(Q[0][0])\n",
        "\n",
        "          states = next_states\n",
        "\n",
        "    return Q\n",
        "\n",
        "\n",
        "\n",
        "#reward(1, 1)\n",
        "\n",
        "#BP_Q = BP_QL(1000, prior_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neQ2Bb2qhSmd"
      },
      "source": [
        "#OSMD Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUjgbimBhWod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962e23cf-415a-406c-eab6-f5baf8b5b874"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.style\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def proj(x):\n",
        "  return np.exp(x)/(np.sum(np.exp(x)))\n",
        "\n",
        "def OSMD(n, eta, delta, T, num_follower_actions=1):\n",
        "  regret_vals = []\n",
        "  utility_vals = []\n",
        "  util = np.random.rand(num_follower_actions, n)\n",
        "  optimal_pure_val = np.max(util)\n",
        "  y = []\n",
        "  while True:\n",
        "    y = np.random.rand(n)\n",
        "    max_index = -math.inf\n",
        "    max_val = -math.inf\n",
        "    for j in range(num_follower_actions):\n",
        "      cur_u = np.dot(y, util[j])\n",
        "      if cur_u > max_val:\n",
        "        max_val = cur_u\n",
        "        max_index = j\n",
        "\n",
        "\n",
        "  for t in range(T):\n",
        "    for i in range(num_follower_actions):\n",
        "      #Compute utility of mixed strategy for each follower action and choose best response\n",
        "      a_0_u = np.dot(util[0], proj(y))\n",
        "      a_1_u = np.dot(util[1], proj(y))\n",
        "      if a_0_u > a_1_u:\n",
        "        follower_action = 0\n",
        "        u = a_0_u\n",
        "      else:\n",
        "        follower_action = 1\n",
        "        u = a_1_u\n",
        "      utility_vals.append(u)\n",
        "      regret_vals.append(u - optimal_pure_val)\n",
        "      #Do OSMD on y\n",
        "      v = np.random.rand(n)\n",
        "      v *= 1/np.sum(v)\n",
        "      x_1 = proj(y + eta*v)\n",
        "      #Compute new utilities based on new mixed strategies\n",
        "      a_0_u_prime = np.dot(util[0], x_1)\n",
        "      a_1_u_prime = np.dot(util[1], x_1)\n",
        "      if a_0_u_prime > a_1_u_prime:\n",
        "        u_prime = a_0_u_prime\n",
        "        follower_action_prime = 0\n",
        "      else:\n",
        "        u_prime = a_1_u_prime\n",
        "        follower_action_prime = 1\n",
        "      if u_prime > u:\n",
        "        y += v*delta\n",
        "      elif u_prime < u:\n",
        "        y += v*(-delta)\n",
        "\n",
        "  print(\"Final utility: \", utility_vals[T-1])\n",
        "  print(\"Final regret: \", regret_vals[T-1])\n",
        "  return regret_vals, utility_vals\n",
        "\n",
        "regret, util = OSMD(10, 1, 1, 150)\n",
        "\n",
        "f1 = plt.figure()\n",
        "ax1 = f1.add_subplot(111)\n",
        "ax1.plot(regret)\n",
        "plt.show()\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-b478daf491cd>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mregret_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility_vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mregret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOSMD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-b478daf491cd>\u001b[0m in \u001b[0;36mOSMD\u001b[0;34m(n, eta, delta, T, num_follower_actions)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}