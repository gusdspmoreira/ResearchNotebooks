{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IipCeTm8YtlJ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "import random, math\n",
        "import scipy\n",
        "from scipy.optimize import minimize_scalar, fsolve\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import sys\n",
        "matplotlib.use('agg')\n",
        "class EXP3:\n",
        "\tdef __init__(self, num_actions, num_iterations=None):\n",
        "\t\tself.num_actions = num_actions\n",
        "\t\tself.belief = np.ones(num_actions, dtype=np.longdouble)\n",
        "\t\tself.belief /= sum(self.belief)\n",
        "\t\tself.gamma = 0\n",
        "\t\tself.action_prob = np.ones(num_actions, dtype=np.longdouble) / num_actions\n",
        "\t\tself.t = 0\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn f\"EXP3\\ngamma={self.gamma}\\naction_prob={self.action_prob}\\n\"\n",
        "\n",
        "\tdef feedback(self, action, reward, state=None):\n",
        "\t\tself.t += 1\n",
        "\n",
        "\t\testimatedReward = reward / self.action_prob[action]\n",
        "\t\tself.belief[action] *= math.exp(estimatedReward * self.gamma / self.num_actions)\n",
        "\t\tself.belief /= sum(self.belief)\n",
        "\n",
        "\t\tself.gamma = min(1, math.sqrt(self.num_actions * 2 * math.log(self.num_actions) / (self.t)))\n",
        "\t\tself.action_prob = (1.0 - self.gamma) * self.belief + self.gamma / self.num_actions\n",
        "\n",
        "class EXP3DH:\n",
        "\tdef __init__(self, num_actions, num_iterations=None, beta=None, b=0.2):\n",
        "\t\tself.num_actions = num_actions\n",
        "\t\t### not essential, but use higher precision just in case\n",
        "\t\tself.loss = np.zeros(num_actions, dtype=np.float128)\n",
        "\t\tself.eps = 0\n",
        "\t\tself.action_prob = np.ones(num_actions, dtype=np.float128) / num_actions\n",
        "\t\tself.t = 0\n",
        "\t\tself.beta = 2 * num_actions if not beta else beta #or 1 for second price auction\n",
        "\t\tself.b = b\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn f\"EXP3-DH\\nbeta={self.beta}\\nb={self.b}\\naction_prob={self.action_prob}\\n\"\n",
        "\n",
        "\tdef compute_action(self):\n",
        "\t\treturn self.action_prob\n",
        "\n",
        "\tdef feedback(self, action, reward, state=None):\n",
        "\t\tself.t += 1\n",
        "\n",
        "\t\testimatedReward = reward / self.action_prob[action]\n",
        "\t\tdiscount = ( (self.t-1)/(self.t) ) ** (self.beta)\n",
        "\t\tself.eps = self.t ** (-self.b)\n",
        "\t\tself.loss *= discount\n",
        "\t\tself.loss[action] += estimatedReward\n",
        "\n",
        "\t\t### here the normalization through minus np.max(self.loss)\n",
        "\t\t### is critical for maintain numerical stability\n",
        "\t\t### while perserve the originial value\n",
        "\t\texp_loss = np.exp( self.loss - np.max(self.loss) )\n",
        "\n",
        "\t\tself.action_prob =  (1-self.eps) * exp_loss / np.sum(exp_loss) + self.eps / self.num_actions\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def general_render(array, name):\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(xlim=(0, len(array)), ylim=(0, np.max(array)))\n",
        "\n",
        "  line, = ax.plot([], [], lw = 1)\n",
        "\n",
        "  def init():\n",
        "    line.set_data([], [])\n",
        "    return line,\n",
        "\n",
        "  def animate(i):\n",
        "    x = range(0, i)\n",
        "    y = array[:i]\n",
        "    line.set_data(x, y)\n",
        "    return line,\n",
        "\n",
        "  if len(array) > 10000:\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=range(0, len(array), len(array)//100), interval=1, blit=True)\n",
        "  else:\n",
        "    anim = FuncAnimation(fig, animate, init_func=init, frames=range(0, len(array)), interval=1000, blit=True)\n",
        "\n",
        "  anim.save(f'{name}.gif', writer=PillowWriter(fps=5))\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "def generate_game(n):\n",
        "  array = []\n",
        "  for i in range(n):\n",
        "    array.append([])\n",
        "    for j in range(n):\n",
        "        array[i].append((random.random()*1/3, random.random()*1/3))\n",
        "\n",
        "  #Randomize important action points and return SSE\n",
        "  c1 = random.randint(0, n-1)\n",
        "  c2 = random.randint(0, n-1)\n",
        "  while c2 == c1:\n",
        "    c2 = random.randint(0, n-1)\n",
        "  r1 = random.randint(0, n-1)\n",
        "  r2 = random.randint(0, n-1)\n",
        "  while r2 == r1:\n",
        "    r2 = random.randint(0, n-1)\n",
        "  array[r1][c1] = (1, 0)\n",
        "  array[r1][c2] = (1/2, 1/2)\n",
        "  array[r2][c1] = (0, 3/4)\n",
        "  array[r2][c2] = (3/4, 1/4)\n",
        "  return array, r1, c2"
      ],
      "metadata": {
        "id": "9OBjM0BITZR1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Stackelberg_n_changes(game_sizes):\n",
        "  iters = []\n",
        "  for n in game_sizes:\n",
        "    simple_SSE, r1, c2 = generate_game(n)\n",
        "    agent_1 = EXP3DH(n)\n",
        "    agent_2 = [EXP3DH(n) for i in range(n)]\n",
        "    SSE = [r1, c2]\n",
        "    cur_iters = 0\n",
        "    while True:\n",
        "      #Leader action choice\n",
        "      action_1 = np.random.choice(list(range(n)), p=agent_1.action_prob.astype('float64'))\n",
        "\n",
        "      #Learning follower action choice\n",
        "      action_2 = np.random.choice(list(range(n)), p=agent_2[action_1].action_prob.astype('float64'))\n",
        "\n",
        "      #Reward model based on action pair taken\n",
        "      reward = simple_SSE[action_1][action_2]\n",
        "      agent_1.feedback(action_1, reward[0])\n",
        "      agent_2[action_1].feedback(action_2, reward[1])\n",
        "\n",
        "      if agent_1.action_prob[SSE[0]] > .8 and agent_2[SSE[0]].action_prob[SSE[1]] > .8:\n",
        "        iters.append(cur_iters)\n",
        "        break\n",
        "\n",
        "      cur_iters += 1\n",
        "\n",
        "  return iters\n",
        "\n",
        "game_sizes = [2, 5, 10, 20, 100, 200, 500]\n",
        "\n",
        "\n",
        "iters = np.asarray(Stackelberg_n_changes(game_sizes))\n",
        "\n",
        "for i in range(4):\n",
        "  iters = np.add(iters, np.asarray(Stackelberg_n_changes(game_sizes)))\n",
        "\n",
        "iters = iters.astype(int) * 1/5\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(game_sizes, iters)\n",
        "ax.set_xlabel('Size of game')\n",
        "ax.set_ylabel('.8 SSE prob iterations')\n",
        "ax.set_title(\"Runtime for EXP3DH algorithm to reach SSE in several game sizes\")\n",
        "plt.savefig(f\"EXP3DH_SSE_runtimes_3.png\")"
      ],
      "metadata": {
        "id": "K3tIWycT-dCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_Stackelberg(n):\n",
        "  #Original 2x2 game that subsequent games are based on\n",
        "  #simple_SSE = [[(1, 0), (1/2, 1/2)], [(0, 3/4), (3/4, 1/4)]]\n",
        "\n",
        "  simple_SSE, r1, c2 = generate_game(n)\n",
        "\n",
        "  #Code for printing game if needed\n",
        "  #for i in simple_SSE:\n",
        "  #  print(i)\n",
        "\n",
        "  #Generate agents by making leader by one EXP3 agent and follower be n EXP3 agents, learning for each leader action\n",
        "  agent_1 = EXP3(n)\n",
        "  agent_2 = [EXP3(n) for i in range(n)]\n",
        "  SSE = [r1, c2]\n",
        "  SSE_probs = []\n",
        "  agent_1_SSE_probs = []\n",
        "  agent_2_SSE_probs = []\n",
        "  for i in range(200000):\n",
        "    #Leader action choice\n",
        "    action_1 = np.random.choice(list(range(n)), p=agent_1.action_prob.astype('float64'))\n",
        "\n",
        "    #Learning follower action choice\n",
        "    action_2 = np.random.choice(list(range(n)), p=agent_2[action_1].action_prob.astype('float64'))\n",
        "\n",
        "    #Reward model based on action pair taken\n",
        "    reward = simple_SSE[action_1][action_2]\n",
        "\n",
        "    if action_1 == SSE[0] and action_2 == SSE[1] and i > 0:\n",
        "      SSE_probs.append(len(SSE_probs)/i)\n",
        "\n",
        "    agent_1.feedback(action_1, reward[0])\n",
        "    agent_2[action_1].feedback(action_2, reward[1])\n",
        "\n",
        "    agent_1_SSE_probs.append(agent_1.action_prob[SSE[0]])\n",
        "    agent_2_SSE_probs.append(agent_2[SSE[0]].action_prob[SSE[1]])\n",
        "\n",
        "  general_render(agent_1_SSE_probs, 'agent_1_SSE_prob')\n",
        "  general_render(agent_2_SSE_probs, 'agent_2_SSE_prob')\n",
        "  if(len(SSE_probs) > 100):\n",
        "    general_render(SSE_probs, 'SSE_prob')\n",
        "\n",
        "  print(agent_1.action_prob[SSE[0]])\n",
        "  print(agent_2[SSE[0]].action_prob[SSE[1]])\n",
        "\n",
        "learn_Stackelberg(10)"
      ],
      "metadata": {
        "id": "YEX2p8EHYylv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}